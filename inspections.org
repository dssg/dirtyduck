#+TITLE: Dirty duck: A triage's guided tour
#+AUTHOR: Center of Data Science for Public Policy
#+EMAIL: adolfo@uchicago.edu
#+STARTUP: showeverything
#+STARTUP: nohideblocks
#+STARTUP: indent
#+PROPERTY: header-args:sql :engine postgresql
#+PROPERTY: header-args:sql+ :dbhost 0.0.0.0
#+PROPERTY: header-args:sql+ :dbport 5434
#+PROPERTY: header-args:sql+ :dbuser food_user
#+PROPERTY: header-args:sql+ :dbpassword some_password
#+PROPERTY: header-args:sql+ :database food
#+PROPERTY: header-args:sql+ :results table drawer
#+PROPERTY: header-args:shell     :results drawer
#+PROPERTY: header-args:ipython   :session :exports both :results raw drawer
#+PROPERTY: header-args:python   :session :exports both :results raw drawer

* Problem description
Inspection the join starts from outcomes (outcome centric) (if you
haven't been inspected, we can not said anything about you)


 /What are you inspecting?/ (people, places, other)
 /How far do you want to predict?/ (e.g. 1 mo, 6mo, 12 mo, etc)
 /How often do you want to update the list?/ (e.g. 1 mo, 6mo, 12 mo, etc)
 /What do you want to optimize for?/ (e.g. efficiency, long term
 compliance, novelty)

The scenario, is the following:  you work for the Chicago's
  government, and you try
  to prioritize your resources (i.e. your inspection workforce), since
  they are limited. So, you will use the data (the same data set,
  remember) for answering the next
  question:

#+begin_quote
Which X facilities are likely to violate some rule in the
  following Y period of time?
#+end_quote

  In this case maybe you are interested not
  in all the violations but in the more grave.



* Creating the labels

Let's remember how the table looks like:

 #+begin_src sql
 \d semantic.events
 #+end_src

 #+RESULTS:
 :RESULTS:
 | Table "semantic.events"                                                                                         |                   |           |
 |-----------------------------------------------------------------------------------------------------------------+-------------------+-----------|
 | Column                                                                                                          | Type              | Modifiers |
 | inspection                                                                                                      | character varying |           |
 | entity_id                                                                                                        | bigint            |           |
 | type                                                                                                            | text              |           |
 | date                                                                                                            | date              |           |
 | risk                                                                                                            | text              |           |
 | result                                                                                                          | text              |           |
 | facility_type                                                                                                    | text              |           |
 | zip_code                                                                                                         | character varying |           |
 | location                                                                                                        | geometry          |           |
 | violations                                                                                                      | jsonb             |           |
 | Indexes:                                                                                                        |                   |           |
 | "events_date_ix" btree (date DESC NULLS LAST)                                                                     |                   |           |
 | "events_entity_ix" btree (entity_id)                                                                               |                   |           |
 | "events_facility_type_ix" btree (facility_type)                                                                     |                   |           |
 | "events_inspection_entity_zip_code_date" btree (inspection DESC NULLS LAST, entity_id, zip_code, date DESC NULLS LAST) |                   |           |
 | "events_inspection_ix" btree (inspection)                                                                         |                   |           |
 | "events_location_gix" gist (location)                                                                             |                   |           |
 | "events_type_ix" btree (type)                                                                                     |                   |           |
 | "events_violations" gin (violations)                                                                             |                   |           |
 | "events_violations_json_path" gin (violations jsonb_path_ops)                                                        |                   |           |
 | "events_zip_code_ix" btree (zip_code)                                                                               |                   |           |
 :END:

We will define two different labels:

- *Which facilities are likely to fail an inspection?*

Facilities who failed an inspection (i.e. =result= = ='fail'=)

- *Which facilities are likely  to fail an inspection with a major  violation?*

Critical violations are coded between =1-14=, serious violations between
=15-29=, everything above =30= is assumed to be a minor violation.

Facilities who failed an inspection (i.e. =result= = ='fail'=) and the
=severity in ('critical', 'serious')=

We will create both labels using the following code:

#+begin_src sql

select inspection, result, array_agg(obj ->>'severity'),
(result = 'fail') as failed,
(result = 'fail' and
('serious' = ANY(array_agg(obj ->> 'severity')) or 'critical' = ANY(array_agg(obj ->> 'severity')))
) as failed_major_violation
from
(select inspection, result, jsonb_array_elements(violations::jsonb) as obj from semantic.events limit 20)
as t1
group by inspection, result

#+end_src

#+RESULTS:
:RESULTS:
| inspection | result             | array_agg                                       | failed | failed_major_violation |
|------------+--------------------+------------------------------------------------+--------+----------------------|
|     100215 | pass w/ conditions | {serious}                                      | f      | f                    |
|     100209 | fail               | {critical,minor,minor,minor,minor,minor,minor} | t      | t                    |
|     104236 | fail               | {serious,serious,minor,minor}                  | t      | t                    |
|     100214 | pass               | {serious}                                      | f      | f                    |
|     100211 | fail               | {critical,serious}                             | t      | t                    |
|     100212 | fail               | {critical,serious}                             | t      | t                    |
|     100213 | pass               | {critical,serious}                             | f      | f                    |
|     100210 | pass               | {NULL}                                         | f      | f                    |
:END:


Let's use the previous query to generate our labels in a new  =inspections= schema

#+begin_src sql :tangle ./src/create_inspections_schema.sql
create schema if not exists inspections;

drop table if exists inspections.events cascade;

create table inspections.events as (
with inspection_labels as (
select inspection, result,
   (result = 'fail') as failed,
   (result = 'fail' and
       ('serious' = ANY(array_agg(obj ->> 'severity')) or 'critical' = ANY(array_agg(obj ->> 'severity')))
   ) as failed_major_violation
from
   (select inspection, result, jsonb_array_elements(violations::jsonb) as obj from semantic.events)
as t1
group by inspection, result
)

select
e.inspection, e.type, e.license_num::INTEGER as entity_id, e.facility_type,
e.zip_code, e.location, e.city, e.date, e.risk, e.result, failed, failed_major_violation
from
semantic.events as e
join
inspection_labels as l
on e.inspection = l.inspection
where zip_code is not null and type is not null
)
#+end_src

#+RESULTS:


Triage has some restrictions (at the current version) about how to
name some of the columns, in specific, our column should include:

- =entity_id=
- =outcome_date=
- =outcome=

=entity_id= an identifier for which the labels are applied to,
=outcome_date= the date at which some outcome was known, =outcome= a
boolean outcome.

Given that do we have two labels, we will create two tables.

#+BEGIN_SRC sql
drop table if exists inspections.failed;

create table inspections.failed as (
select
entity_id,
date as outcome_date,
failed as outcome
from inspections.events
);


drop table if exists inspections.failed_major_violation;

create table inspections.failed_major_violation as (
select
entity_id,
date as outcome_date,
failed_major_violation as outcome
from inspections.events
);



#+END_SRC

#+RESULTS:


Also, we will need a *states* table. The table must have columns
=entity_id, state, start_time, end_time=. The states table allows us to only
include rows in your matrices in a specific state. In our case we only want
to inspect *active* facilities,

#+BEGIN_SRC sql

drop table if exists inspections.active_facilities;


create table inspections.active_facilities as (
with dates as (
select
license_num::INTEGER as entity_id,
min(date) over (partition by license_num) as start_time,
(case when
results in ('Out of Business', 'Business Not Located')
then
date
else
NULL
end) as end_time
from inspections
where zip is not null and license_num is not null
)

select
distinct on (entity_id)
entity_id, 'active'::VARCHAR  as state, start_time, coalesce(end_time, '2020-12-31'::date) as end_time
from dates
);
#+END_SRC

#+RESULTS:



* Modeling using Machine Learning

It is time of getting all the previous steps and put them
together. Don't worry, actually we are done coding. =triage= provides
you with a configuration file for specifying the experiment that we
want to run.

#+BEGIN_SRC yaml :tangle src/inspections_test.yaml
config_version: 'v3'

# EXPERIMENT METADATA
# model_comment (optional) will end up in the model_comment column of the
# models table for each model created in this experiment
model_comment: 'inspections_test'
#+END_SRC

#+BEGIN_SRC yaml :tangle src/inspections_test.yaml
temporal_config:
    feature_start_time: '2014-02-08'
    feature_end_time: '2016-10-17'
    label_start_time: '2014-02-08'
    label_end_time: '2016-10-17'
    model_update_frequency: '1y'
    training_label_timespans: ['1month']
    training_as_of_date_frequencies: '1month'
    max_training_histories: '1y'
    test_durations: '1d'
    test_label_timespans: ['3month']
    test_as_of_date_frequencies: '1month'
#+END_SRC


#+BEGIN_SRC yaml :tangle src/inspections_test.yaml
events_table: inspections.failed
#+END_SRC


Each entry describes a collate.SpacetimeAggregation object, and the
arguments needed to create it. Generally, each of these entries controls
the features from one source table, though in the case of multiple groups
may result in multiple output tables

#+BEGIN_SRC yaml :tangle src/inspections_test.yaml
feature_aggregations:
    -
        # prefix given to the resultant tables
        prefix: 'inspections'
        # from_obj is usually a source table but can be an expression, such as
        # a join (ie 'cool_stuff join other_stuff using (stuff_id)')
        from_obj: 'inspections.events'
        # The date column to use for specifying which records to include
        # in temporal features. It is important that the column used specifies
        # the date at which the event is known about, which may be different
        # from the date the event happened.
        knowledge_date_column: 'date'

        # top-level imputation rules that will apply to all aggregates functions
        # can also specify categoricals_imputation or array_categoricals_imputation
        #
        # You must specified at least one of the top-level or feature-level imputation
        # to cover ever feature being defined.
        categoricals_imputation:
            # The `all` rule will apply to all aggregation functions, unless over-
            # ridden by a more specific one
            all:
                # every imputation rule must have a `type` parameter, while some
                # (like 'constant') have other required parameters (`value` here)
                type: 'zero'

        # aggregates and categoricals define the actual features created. So
        # at least one is required
        #
        # Aggregates of numerical columns. Each quantity is a number of some
        # sort, and the list of metrics are applied to each quantity
        # Categorical features. The column given can be of any type, but the
        # choices must comparable to that type for equality within SQL
        # The result will be one feature for each choice/metric combination
        categoricals:
            -
                column: 'type'
                # note that we haven't specified a top level `categoricals_imputation`
                # set of rules, so we have to include feature-specific imputation
                # rules for both of our categoricals here.
                choice_query: 'select distinct type from inspections.events where type is not null'
                metrics:
                    - 'sum'
        # The time intervals over which to aggregate features
        intervals:
            - '1y'

        # A list of different columns to separately group by
        groups:
            - 'entity_id'
            - 'zip_code'
#+END_SRC

#+BEGIN_SRC yaml :tangle src/inspections_test.yaml

# FEATURE GROUPING
# define how to group features and generate combinations
# feature_group_definition allows you to create groups/subset of your features
# by different criteria.
# for instance,
# - 'tables' allows you to send a list of collate feature tables (collate builds these by appending 'aggregation_imputed' to the prefix)
# - 'prefix' allows you to specify a list of feature name prefixes
feature_group_definition:
   prefix: ['inspections']
#+END_SRC


#+BEGIN_SRC yaml :tangle src/inspections_test.yaml
# STATE MANAGEMENT (optional)
# If you want to only include rows in your matrices in a specific state,
# provide:
# 1. a dense state table that defines when entities were in specific states
#   should have columns entity_id/state/start/end
# 2. a list of state filtering SQL clauses to iterate through. Assuming the
#   states are boolean columns (the experiment will convert the one you pass in
#   to this format), write a SQL expression for each state
#   configuration you want, ie '(permitted OR suspended) AND licensed'
state_config:
    table_name: 'inspections.active_facilities'
    state_filters:
        - 'active'
#+END_SRC


#+BEGIN_SRC yaml :tangle src/inspections_test.yaml
# USER METADATA
# These are arbitrary keys/values that you can have Triage apply to the
# metadata for every matrix in the experiment. Any keys you include here can
# be used in the 'model_group_keys' below. For example, if you run experiments
# that share a temporal configuration but that use different label definitions
# (say, labeling building inspections with *any* violation as positive or
# labeling only building inspections with severe health and safety violations
# as positive), you can use the user metadata keys to indicate that the matrices
# from these experiments have different labeling criteria. The matrices from the
# two experiments will have different filenames (and not be overwritten or
# inappropriately reused), and if you add the label_definition key to the model
# group keys, models made on different label definition will have different
# groups. In this way, user metadata can be used to expand Triage beyond its
# explicitly supported functionality.
user_metadata:
  label_definition: 'failed'
#+END_SRC

#+BEGIN_SRC yaml :tangle src/inspections_test.yaml
model_group_keys:
    - 'label_definition'
#+END_SRC

#+BEGIN_SRC yaml :tangle src/inspections_test.yaml
grid_config:
    'sklearn.ensemble.ExtraTreesClassifier':
        n_estimators: [100]
        criterion: [gini, entropy]
        max_depth: [1,5,10,20,50]
        max_features: [sqrt]
        min_samples_split: [2,5,10]
#+END_SRC

#+BEGIN_SRC yaml :tangle src/inspections_test.yaml
scoring:
    sort_seed: 5
    metric_groups:
        -
            metrics: [precision@, recall@]
            thresholds:
                percentiles: [5.0, 10.0]
                top_n: [5, 10]
        -
            metrics: [f1]
        -
            metrics: [fbeta@]
            parameters:
                -
                    beta: 0.75
                -
                    beta: 1.25

#+END_SRC


#+BEGIN_SRC yaml :tangle src/inspections_test.yaml
# INDIVIDUAL IMPORTANCES
# How feature importances for individuals should be computed
# There are two variables here:
# methods: Refer to *how to compute* individual importances.
#   Each entry in this list should represent a different method.
#   Available methods are in the catwalk library's:
#   `catwalk.individual_importance.CALCULATE_STRATEGIES` list
#   Will default to 'uniform', or just the global importances.
#
# n_ranks: The number of top features per individual to compute importances for
#   Will default to 5
#
# This entire section can be left blank,
# in which case the defaults will be used.
individual_importance:
    methods: ['uniform']
    n_ranks: 5
#+END_SRC



** â–¶ TODO Creating a simple experiment

Using the same subset as before, we will try one of the simplest
machine learning algorithms: a Decision Tree Classifier

We began with this data set:

Our train matrices look like:

And the test matrices:

We can check the results of the experiment here:


Now let's do a real model

** Defining a baseline
It is always a good idea define a baseline, we will use

** The grid

** How can I pick the best one?


We are working in ...

But meanwhile, you can try the following
