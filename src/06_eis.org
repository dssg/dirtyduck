#+STARTUP: showeverything
#+STARTUP: nohideblocks
#+STARTUP: indent
#+STARTUP: align
#+STARTUP: inlineimages
#+STARTUP: latexpreview
#+PROPERTY: header-args:sql :engine postgresql
#+PROPERTY: header-args:sql+ :dbhost 0.0.0.0
#+PROPERTY: header-args:sql+ :dbport 5434
#+PROPERTY: header-args:sql+ :dbuser food_user
#+PROPERTY: header-args:sql+ :dbpassword some_password
#+PROPERTY: header-args:sql+ :database food
#+PROPERTY: header-args:sql+ :results table drawer
#+PROPERTY: header-args:sql+ :cmdline -q
#+PROPERTY: header-args:sh  :results verbatim org
#+PROPERTY: header-args:sh+ :prologue exec 2>&1 :epilogue :
#+PROPERTY: header-args:ipython   :session Food_inspections
#+PROPERTY: header-args:ipython+ :results raw drawer
#+OPTIONS: broken-links:mark
#+OPTIONS: tasks:todo
#+OPTIONS: LaTeX:t

* An Early Intervention System

** Problem description

=triage= is designed to also build early warning systems (also called early intervention, EIS).
While there are  several differences between modeling early warnings and inspection
 prioritization, perhaps the biggest is that  the /entity/ is active
(i.e. it is doing stuff for which
 an outcome will happen) in EIS but passive (i.e. inspected)
 in *inspection prioritization*. Among other things, this difference
affects the way the /outcome/ is built.

Here's the question we want to answer:

#+begin_quote
Will my restaurant be inspected in the
/next X period of time?/
#+end_quote

Where $X$ could be 3 days, 2 months, 1 year,
etc.

  Knowing the answer to this question enables you (as the restaurant
  owner or manager) to prepare for the inspection.


** What are the labels? What are the outcomes?

The trick to note is that on any given day there are two possible outcomes:
/the facility was inspected/ and /the facility wasn't inspected/.
Our /outcomes/ table will be larger than in the inspection prioritization example
because we need an /outcome/ for every /active/ facility on every date.
The following image tries to exemplify this reasoning:


#+NAME: fig:outcomes-inspections
#+CAPTION: The image shows three facilities, and next to each, a temporal line with 6 days (0-5). Each dot represents the event (whether an inspection happened). Yellow means the inspection happened (=TRUE= outcome) and blue means it didn't (=FALSE= outcome). Each facility in the image had two inspections, six in total.
#+ATTR_ORG: :width 600 :height 400
#+ATTR_HTML: :width 600 :height 400
#+ATTR_LATEX: :width 400 :height 300
[[./images/outcomes-eis.png]]

Fortunately, =triage= will help us to create this table. The /cohort/
table is the same as the /cohort/ table in the inspection case.


First the usual stuff. Note that we are changing =model_comment= and
=label_definition= (remember that this is used for generating the
/hash/ that differentiates models and model groups).

#+BEGIN_SRC yaml :tangle ../triage/experiment_config/eis_01.yaml
config_version: 'v5'

model_comment: 'eis'

user_metadata:
  label_definition: 'inspected'
  experiment_type: 'eis'
  description: |
    Experiment 01
  purpose: 'exploring'
  org: 'DSaPP'
  team: 'Tutorial'
  author: 'Your name here'
#+END_SRC

For the labels the query is pretty simple, if the facility showed in
the data, it will get a /positive/ outcome, if not they will get a /negative/ outcome

#+BEGIN_SRC yaml :tangle ../triage/experiment_config/eis_01.yaml
label_config:
  query: |
    select
    entity_id,
    True::integer as outcome
    from semantic.events
    where '{as_of_date}'::timestamp <= date
    and date < '{as_of_date}'::timestamp + interval '{label_timespan}'
    group by entity_id
  include_missing_labels_in_train_as: False
  name: 'inspected'
#+END_SRC

Note the two introduced changes in this block, first, the /outcome/ is
=True= , because all our observations represent /inspected/ facilities
(see discussion above and in particular previous image), second, we
added the line =include_missing_labels_in_train_as: False=. This line
tells =triage= to incorporate all the missing facilities with =False=  as
the /outcome/.

As stated we will use the same configuration block for /cohorts/ that we
used in inspections:

#+BEGIN_SRC yaml :tangle ../triage/experiment_config/eis_01.yaml
cohort_config:
  query: |
    select entity_id
    from semantic.entities
    where
    tsrange(start_time, end_time, '[]') @> {as_of_date}
  name: 'active_facilities'
#+END_SRC


** Modeling Using Machine Learning

We need to specify the temporal configuration too

**** Temporal configuration
#+BEGIN_SRC yaml :tangle ../triage/experiment_config/eis_01.yaml
temporal_config:
    feature_start_time: '2010-01-04'
    feature_end_time: '2018-03-13'
    label_start_time: '2015-02-01'
    label_end_time: '2018-03-13'

    model_update_frequency: '1y'
    training_label_timespans: ['1month']
    training_as_of_date_frequencies: '1month'

    test_durations: '1month'
    test_label_timespans: ['1month']
    test_as_of_date_frequencies: '1month'

    max_training_histories: '5y'
#+END_SRC


As before, you can generate the image of the temporal blocks:


#+BEGIN_SRC sh
./tutorial.sh triage --config_file eis_01.yaml show-temporal-blocks
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
Using the config file /triage/experiment_config/eis_01.yaml
The output (matrices and models) of this experiment will be stored in triage/output
Using data stored in postgresql://food_user:some_password@food_db/food
The experiment will utilize any preexisting matrix or model: False
Creating experiment object
Experiment loaded
Generating temporal blocks image
Image stored in:
/triage/output/images/eis.svg
/usr/local/lib/python3.6/site-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use "pip install psycopg2-binary" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.
  """)
#+End_src

#+CAPTION: Temporal blocks for the Early Warning System. We want to predict the most likely facilities to be inspected in the following month.
#+ATTR_ORG: :width 600 :height 400
#+ATTR_HTML: :width 600 :height 600
#+ATTR_LATEX: :width 400 :height 300
[[./images/eis.png]]

**** Features

Regarding the features, we will use the same ones that were used in [[file:inspections.org][inspections prioritization]]:

#+BEGIN_SRC yaml :tangle ../triage/experiment_config/eis_01.yaml
feature_aggregations:
    -
        prefix: 'inspections'
        from_obj: 'semantic.events'
        knowledge_date_column: 'date'

        categoricals_imputation:
            all:
                type: 'zero'

        categoricals:
            -   # inspection type
                column: 'type'
                choice_query: 'select distinct type from semantic.events'
                metrics:
                    - 'sum'
                    - 'avg'

        intervals:
            - '2y'
            - '1y'
            - '6month'
            - '3month'

        groups:
            - 'entity_id'
            - 'zip_code'

    -
        prefix: 'risks'
        from_obj: 'semantic.events'
        knowledge_date_column: 'date'

        categoricals_imputation:
            all:
                type: 'zero'

        categoricals:
            -   # Facility's Risk
                column: 'risk'
                choice_query: 'select distinct risk from semantic.events'
                metrics:
                    - 'sum'
                    - 'avg'

        intervals:
            - '2y'
            - '1y'
            - '6month'
            - '3month'

        groups:
            - 'entity_id'
            - 'zip_code'
            - 'facility_type'


    -
        prefix: 'results'
        from_obj: 'semantic.events'
        knowledge_date_column: 'date'

        categoricals_imputation:
            all:
                type: 'zero'

        categoricals:
            -   # Result of previous inspections
                column: 'result'
                choice_query: 'select distinct result from semantic.events'
                metrics:
                    - 'sum'
                    - 'avg'

        intervals:
            - '2y'
            - '1y'
            - '6month'
            - '3month'

        groups:
            - 'entity_id'
            - 'zip_code'
            - 'facility_type'

#+END_SRC

We declare that we want to use all possible feature-group combinations for training:

#+BEGIN_SRC yaml :tangle ../triage/experiment_config/eis_01.yaml
feature_group_definition:
   prefix: ['inspections', 'results', 'risks']

feature_group_strategies: ['all', 'leave-one-in', 'leave-one-out']
#+END_SRC

i.e. =all= will train models with all the features groups,
=leave-one-in= will use only one of the feature groups for traning, and
lastly, =leave-one-out= will train the model with all the features
except one.

**** Algorithm and hyperparameters

We will collapse the baseline (=DummyClassifier=) and the exploratory configuration together:

#+BEGIN_SRC yaml :tangle ../triage/experiment_config/eis_01.yaml
grid_config:
    'sklearn.tree.DecisionTreeClassifier':
        max_depth: [1,null]
    'sklearn.ensemble.RandomForestClassifier':
        max_features: ['sqrt']
        criterion: ['gini']
        n_estimators: [1000]
        min_samples_leaf: [1]
        min_samples_split: [50]
        class_weight: ['balanced']
    'sklearn.dummy.DummyClassifier':
        strategy: [prior,uniform, most_frequent]
#+END_SRC

=triage= will create *42* /model groups/: 6 algorithms and
hyperparameters (2 =DecisionTreeClassifier=, 1
=RandomForestClassifier=, 3 =DummyClassifier=) \times 7 features groups (1
=all=, 3 =leave-one-in=, 3 =leave-one-out=). The total number of /models/
is double that (we have 2 time blocks, so *84*).


#+BEGIN_SRC yaml :tangle ../triage/experiment_config/eis_01.yaml
scoring:
    sort_seed: 1234
    testing_metric_groups:
        -
            metrics: ['precision@', 'recall@']
            thresholds:
                percentiles: [1.0, 2.0, 5.0, 10.0, 25.0, 50.0, 75.0, 95.0, 100.0]
                top_n: [5, 10, 25, 50, 75, 100, 150, 200, 300, 500, 1000, 2000]
    training_metric_groups:
      -
        metrics: [accuracy]
      -
        metrics: [precision@, recall@]
        thresholds:
            percentiles: [1.0, 2.0, 5.0, 10.0, 25.0, 50.0, 75.0, 95.0, 100.0]
            top_n: [5, 10, 25, 50, 75, 100, 150, 200, 300, 500, 1000, 2000]
#+END_SRC

As a last step, we validate that the configuration file is correct:

#+BEGIN_SRC sh
./tutorial.sh triage --config_file eis_01.yaml validate
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
Using the config file /triage/experiment_config/eis_01.yaml
The output (matrices and models) of this experiment will be stored in triage/output
Using data stored in postgresql://food_user:some_password@food_db/food
The experiment will utilize any preexisting matrix or model: False
Creating experiment object
Experiment loaded
Validating experiment's configuration
Experiment validation ran to completion with no errors

----TIME SPLIT SUMMARY----

Number of time splits: 3
Split index 0:
            Training as_of_time_range: 2015-02-13 00:00:00 to 2015-12-13 00:00:00 (11 total)
            Testing as_of_time range: 2016-01-13 00:00:00 to 2016-01-13 00:00:00 (1 total)


Split index 1:
            Training as_of_time_range: 2015-02-13 00:00:00 to 2016-12-13 00:00:00 (23 total)
            Testing as_of_time range: 2017-01-13 00:00:00 to 2017-01-13 00:00:00 (1 total)


Split index 2:
            Training as_of_time_range: 2015-02-13 00:00:00 to 2017-12-13 00:00:00 (35 total)
            Testing as_of_time range: 2018-01-13 00:00:00 to 2018-01-13 00:00:00 (1 total)


For more detailed information on your time splits, inspect the experiment `split_definitions` property

           The experiment configuration doesn't contain any obvious errors.
           Any error that occurs from now on, possibly will be related to hit the maximum
           number of columns allowed or collision in
           the column names, both due to PostgreSQL limitations.

The experiment looks in good shape. May the force be with you!
/usr/local/lib/python3.6/site-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use "pip install psycopg2-binary" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.
  """)
/usr/local/lib/python3.6/site-packages/sqlalchemy/sql/base.py:291: SAWarning: Can't validate argument 'autoload_from'; can't locate any SQLAlchemy dialect named 'autoload'
  (k, dialect_name))
#+END_SRC



#+BEGIN_EXAMPLE sh
./tutorial.sh triage --config_file eis_01.yaml --no-replace --debug run
#+END_EXAMPLE

This will take a *lot* amount of time (on my computer took 3h 42m),
so, grab your coffee, chat with
your coworkers, check your email, or read the [[https://dssg.uchicago.edu/blog][DSSG blog]].
It's taking that long for several reasons:

1. There are a lot of models, parameters, etc.
2. We are running in serial mode (i.e. not in parallel).
3. The database is running on your laptop.

You can solve 2 and 3. For the second point you could use the =docker=
container that has the multicore option enabled. For 3, I recommed you
to use a PostgreSQL database in the cloud, such as Amazon's
*PostgreSQL RDS*.

After the experiment finishes, we can create the following table:

#+BEGIN_SRC sql
with features_groups as (
select model_group_id, split_part(unnest(feature_list), '_', 1) as feature_groups
from model_metadata.model_groups
),

features_arrays as (
select model_group_id, array_agg(distinct feature_groups) as feature_groups
from features_groups
group by model_group_id
)

select
model_group_id,
model_type,
hyperparameters,
feature_groups,
array_agg(model_id) as models
from model_metadata.models
join features_arrays using(model_group_id)
where model_comment = 'eis'
group by model_group_id, model_type, hyperparameters, feature_groups order by model_group_id
#+END_SRC

#+RESULTS:
:RESULTS:
| model_group_id | model_type                               | hyperparameters                                                                                                                          | feature_groups               | models   |
|--------------+-----------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------+----------|
|           10 | sklearn.tree.DecisionTreeClassifier     | {"max_depth": 1}                                                                                                                          | {inspections,results,risks} | {61,19}  |
|           11 | sklearn.tree.DecisionTreeClassifier     | {"max_depth": null}                                                                                                                       | {inspections,results,risks} | {62,20}  |
|           12 | sklearn.ensemble.RandomForestClassifier | {"criterion": "gini", "class_weight": "balanced", "max_features": "sqrt", "n_estimators": 1000, "min_samples_leaf": 1, "min_samples_split": 50} | {inspections,results,risks} | {63,21}  |
|           13 | sklearn.dummy.DummyClassifier           | {"strategy": "prior"}                                                                                                                    | {inspections,results,risks} | {64,22}  |
|           14 | sklearn.dummy.DummyClassifier           | {"strategy": "uniform"}                                                                                                                  | {inspections,results,risks} | {65,23}  |
|           15 | sklearn.dummy.DummyClassifier           | {"strategy": "most_frequent"}                                                                                                             | {inspections,results,risks} | {66,24}  |
|           16 | sklearn.tree.DecisionTreeClassifier     | {"max_depth": 1}                                                                                                                          | {inspections}               | {67,25}  |
|           17 | sklearn.tree.DecisionTreeClassifier     | {"max_depth": null}                                                                                                                       | {inspections}               | {68,26}  |
|           18 | sklearn.ensemble.RandomForestClassifier | {"criterion": "gini", "class_weight": "balanced", "max_features": "sqrt", "n_estimators": 1000, "min_samples_leaf": 1, "min_samples_split": 50} | {inspections}               | {69,27}  |
|           19 | sklearn.dummy.DummyClassifier           | {"strategy": "prior"}                                                                                                                    | {inspections}               | {70,28}  |
|           20 | sklearn.dummy.DummyClassifier           | {"strategy": "uniform"}                                                                                                                  | {inspections}               | {71,29}  |
|           21 | sklearn.dummy.DummyClassifier           | {"strategy": "most_frequent"}                                                                                                             | {inspections}               | {72,30}  |
|           22 | sklearn.tree.DecisionTreeClassifier     | {"max_depth": 1}                                                                                                                          | {results}                   | {73,31}  |
|           23 | sklearn.tree.DecisionTreeClassifier     | {"max_depth": null}                                                                                                                       | {results}                   | {74,32}  |
|           24 | sklearn.ensemble.RandomForestClassifier | {"criterion": "gini", "class_weight": "balanced", "max_features": "sqrt", "n_estimators": 1000, "min_samples_leaf": 1, "min_samples_split": 50} | {results}                   | {75,33}  |
|           25 | sklearn.dummy.DummyClassifier           | {"strategy": "prior"}                                                                                                                    | {results}                   | {76,34}  |
|           26 | sklearn.dummy.DummyClassifier           | {"strategy": "uniform"}                                                                                                                  | {results}                   | {77,35}  |
|           27 | sklearn.dummy.DummyClassifier           | {"strategy": "most_frequent"}                                                                                                             | {results}                   | {78,36}  |
|           28 | sklearn.tree.DecisionTreeClassifier     | {"max_depth": 1}                                                                                                                          | {risks}                     | {79,37}  |
|           29 | sklearn.tree.DecisionTreeClassifier     | {"max_depth": null}                                                                                                                       | {risks}                     | {80,38}  |
|           30 | sklearn.ensemble.RandomForestClassifier | {"criterion": "gini", "class_weight": "balanced", "max_features": "sqrt", "n_estimators": 1000, "min_samples_leaf": 1, "min_samples_split": 50} | {risks}                     | {81,39}  |
|           31 | sklearn.dummy.DummyClassifier           | {"strategy": "prior"}                                                                                                                    | {risks}                     | {82,40}  |
|           32 | sklearn.dummy.DummyClassifier           | {"strategy": "uniform"}                                                                                                                  | {risks}                     | {83,41}  |
|           33 | sklearn.dummy.DummyClassifier           | {"strategy": "most_frequent"}                                                                                                             | {risks}                     | {84,42}  |
|           34 | sklearn.tree.DecisionTreeClassifier     | {"max_depth": 1}                                                                                                                          | {results,risks}             | {85,43}  |
|           35 | sklearn.tree.DecisionTreeClassifier     | {"max_depth": null}                                                                                                                       | {results,risks}             | {86,44}  |
|           36 | sklearn.ensemble.RandomForestClassifier | {"criterion": "gini", "class_weight": "balanced", "max_features": "sqrt", "n_estimators": 1000, "min_samples_leaf": 1, "min_samples_split": 50} | {results,risks}             | {87,45}  |
|           37 | sklearn.dummy.DummyClassifier           | {"strategy": "prior"}                                                                                                                    | {results,risks}             | {88,46}  |
|           38 | sklearn.dummy.DummyClassifier           | {"strategy": "uniform"}                                                                                                                  | {results,risks}             | {89,47}  |
|           39 | sklearn.dummy.DummyClassifier           | {"strategy": "most_frequent"}                                                                                                             | {results,risks}             | {90,48}  |
|           40 | sklearn.tree.DecisionTreeClassifier     | {"max_depth": 1}                                                                                                                          | {inspections,risks}         | {91,49}  |
|           41 | sklearn.tree.DecisionTreeClassifier     | {"max_depth": null}                                                                                                                       | {inspections,risks}         | {92,50}  |
|           42 | sklearn.ensemble.RandomForestClassifier | {"criterion": "gini", "class_weight": "balanced", "max_features": "sqrt", "n_estimators": 1000, "min_samples_leaf": 1, "min_samples_split": 50} | {inspections,risks}         | {93,51}  |
|           43 | sklearn.dummy.DummyClassifier           | {"strategy": "prior"}                                                                                                                    | {inspections,risks}         | {94,52}  |
|           44 | sklearn.dummy.DummyClassifier           | {"strategy": "uniform"}                                                                                                                  | {inspections,risks}         | {95,53}  |
|           45 | sklearn.dummy.DummyClassifier           | {"strategy": "most_frequent"}                                                                                                             | {inspections,risks}         | {96,54}  |
|           46 | sklearn.tree.DecisionTreeClassifier     | {"max_depth": 1}                                                                                                                          | {inspections,results}       | {97,55}  |
|           47 | sklearn.tree.DecisionTreeClassifier     | {"max_depth": null}                                                                                                                       | {inspections,results}       | {98,56}  |
|           48 | sklearn.ensemble.RandomForestClassifier | {"criterion": "gini", "class_weight": "balanced", "max_features": "sqrt", "n_estimators": 1000, "min_samples_leaf": 1, "min_samples_split": 50} | {inspections,results}       | {99,57}  |
|           49 | sklearn.dummy.DummyClassifier           | {"strategy": "prior"}                                                                                                                    | {inspections,results}       | {100,58} |
|           50 | sklearn.dummy.DummyClassifier           | {"strategy": "uniform"}                                                                                                                  | {inspections,results}       | {101,59} |
|           51 | sklearn.dummy.DummyClassifier           | {"strategy": "most_frequent"}                                                                                                             | {inspections,results}       | {102,60} |
:END:


Let's check the performance of /model group/ 12 (a Random Forest
with 1,000 trees and all the features):

#+BEGIN_SRC sql
select
model_id, evaluation_start_time,
metric || parameter as metric,
value,
num_labeled_examples,
num_labeled_above_threshold,
num_positive_labels
from test_results.evaluations
where model_id in (21, 63)
and metric || parameter = 'precision@25_abs'
order by num_labeled_above_threshold asc,
metric || parameter
#+END_SRC

#+RESULTS:
:RESULTS:
| model_id | evaluation_start_time | metric          | value | num_labeled_examples | num_labeled_above_threshold | num_positive_labels |
|---------+---------------------+-----------------+-------+--------------------+--------------------------+-------------------|
|      21 | 2015-12-13 00:00:00 | precision@25_abs |  0.84 |              18668 |                       25 |               790 |
|      63 | 2016-12-13 00:00:00 | precision@25_abs |  0.88 |              19358 |                       25 |               958 |
:END:

Using the model =21= we can generate a list of places to be alerted:

#+BEGIN_SRC sql
select *
from test_results.predictions
where model_id = 21
order by score desc
limit 25
#+END_SRC

#+RESULTS:
:RESULTS:
| model_id | entity_id | as_of_date            |              score | label_value | rank_abs | rank_pct | matrix_uuid                       | test_label_timespan |
|---------+----------+---------------------+--------------------+------------+---------+---------+----------------------------------+-------------------|
|      21 |    25854 | 2015-12-13 00:00:00 | 0.9484136677797923 |          1 | [NULL]  | [NULL]  | 2e85917732c60eb208f2d052a9e4fe60 | 1 mon             |
|      21 |     3019 | 2015-12-13 00:00:00 | 0.9436469079812785 |          1 | [NULL]  | [NULL]  | 2e85917732c60eb208f2d052a9e4fe60 | 1 mon             |
|      21 |    13792 | 2015-12-13 00:00:00 | 0.9419947954600382 |          1 | [NULL]  | [NULL]  | 2e85917732c60eb208f2d052a9e4fe60 | 1 mon             |
|      21 |    17502 | 2015-12-13 00:00:00 | 0.9312907285784298 |          1 | [NULL]  | [NULL]  | 2e85917732c60eb208f2d052a9e4fe60 | 1 mon             |
|      21 |     2466 | 2015-12-13 00:00:00 | 0.9243102785460537 |          1 | [NULL]  | [NULL]  | 2e85917732c60eb208f2d052a9e4fe60 | 1 mon             |
|      21 |    10382 | 2015-12-13 00:00:00 | 0.9206737935216891 |          1 | [NULL]  | [NULL]  | 2e85917732c60eb208f2d052a9e4fe60 | 1 mon             |
|      21 |    24297 | 2015-12-13 00:00:00 | 0.9200144700893553 |          0 | [NULL]  | [NULL]  | 2e85917732c60eb208f2d052a9e4fe60 | 1 mon             |
|      21 |      143 | 2015-12-13 00:00:00 | 0.9199824040634403 |          0 | [NULL]  | [NULL]  | 2e85917732c60eb208f2d052a9e4fe60 | 1 mon             |
|      21 |    29174 | 2015-12-13 00:00:00 | 0.9141830973245461 |          1 | [NULL]  | [NULL]  | 2e85917732c60eb208f2d052a9e4fe60 | 1 mon             |
|      21 |     1816 | 2015-12-13 00:00:00 | 0.9116947701339125 |          1 | [NULL]  | [NULL]  | 2e85917732c60eb208f2d052a9e4fe60 | 1 mon             |
|      21 |    15936 | 2015-12-13 00:00:00 |  0.910420304141231 |          1 | [NULL]  | [NULL]  | 2e85917732c60eb208f2d052a9e4fe60 | 1 mon             |
|      21 |     4019 | 2015-12-13 00:00:00 | 0.9101049552562018 |          1 | [NULL]  | [NULL]  | 2e85917732c60eb208f2d052a9e4fe60 | 1 mon             |
|      21 |     8835 | 2015-12-13 00:00:00 |  0.909611746159294 |          1 | [NULL]  | [NULL]  | 2e85917732c60eb208f2d052a9e4fe60 | 1 mon             |
|      21 |    30294 | 2015-12-13 00:00:00 | 0.9053675764327609 |          0 | [NULL]  | [NULL]  | 2e85917732c60eb208f2d052a9e4fe60 | 1 mon             |
|      21 |    27955 | 2015-12-13 00:00:00 | 0.9046749372346382 |          1 | [NULL]  | [NULL]  | 2e85917732c60eb208f2d052a9e4fe60 | 1 mon             |
|      21 |    27956 | 2015-12-13 00:00:00 | 0.9046749372346382 |          1 | [NULL]  | [NULL]  | 2e85917732c60eb208f2d052a9e4fe60 | 1 mon             |
|      21 |    13783 | 2015-12-13 00:00:00 | 0.9015988243973061 |          1 | [NULL]  | [NULL]  | 2e85917732c60eb208f2d052a9e4fe60 | 1 mon             |
|      21 |     8760 | 2015-12-13 00:00:00 | 0.8936300434147297 |          1 | [NULL]  | [NULL]  | 2e85917732c60eb208f2d052a9e4fe60 | 1 mon             |
|      21 |    27071 | 2015-12-13 00:00:00 | 0.8889020831400419 |          1 | [NULL]  | [NULL]  | 2e85917732c60eb208f2d052a9e4fe60 | 1 mon             |
|      21 |    27073 | 2015-12-13 00:00:00 | 0.8889020831400419 |          1 | [NULL]  | [NULL]  | 2e85917732c60eb208f2d052a9e4fe60 | 1 mon             |
|      21 |     1381 | 2015-12-13 00:00:00 | 0.8856752816172949 |          0 | [NULL]  | [NULL]  | 2e85917732c60eb208f2d052a9e4fe60 | 1 mon             |
|      21 |    23063 | 2015-12-13 00:00:00 | 0.8838436286127278 |          1 | [NULL]  | [NULL]  | 2e85917732c60eb208f2d052a9e4fe60 | 1 mon             |
|      21 |     1355 | 2015-12-13 00:00:00 | 0.8828514873911587 |          1 | [NULL]  | [NULL]  | 2e85917732c60eb208f2d052a9e4fe60 | 1 mon             |
|      21 |    19735 | 2015-12-13 00:00:00 | 0.8806239565602527 |          1 | [NULL]  | [NULL]  | 2e85917732c60eb208f2d052a9e4fe60 | 1 mon             |
|      21 |     6959 | 2015-12-13 00:00:00 | 0.8736905232325605 |          1 | [NULL]  | [NULL]  | 2e85917732c60eb208f2d052a9e4fe60 | 1 mon             |
:END:


**** Feature Importances

Which /features/ were important for the picked model?

#+BEGIN_SRC sql
select * from test_results.feature_importances
where model_id = 21
order by feature_importance desc
limit 25
#+END_SRC

#+RESULTS:
:RESULTS:
| model_id | feature                                     | feature_importance | rank_abs |  rank_pct |
|---------+---------------------------------------------+-------------------+---------+----------|
|      21 | risks_facility_type_2y_risk_high_avg              |      0.0174800538 |       1 | 0.003125 |
|      21 | results_entity_id_3month_result_fail_avg          |      0.0133257966 |       2 |  0.00625 |
|      21 | risks_facility_type_1y_risk_high_avg              |      0.0131853305 |       3 | 0.009375 |
|      21 | risks_facility_type_2y_risk_high_sum              |      0.0131239412 |       4 |   0.0125 |
|      21 | results_entity_id_6month_result_pass_avg          |      0.0129326733 |       5 | 0.015625 |
|      21 | risks_facility_type_6month_risk_high_avg          |      0.0124089975 |       6 |  0.01875 |
|      21 | results_facility_type_2y_result_pass_avg          |      0.0119138182 |       7 | 0.021875 |
|      21 | risks_entity_id_2y_risk_high_sum                  |       0.011606308 |       8 |    0.025 |
|      21 | results_entity_id_6month_result_pass_sum          |      0.0115504677 |       9 | 0.028125 |
|      21 | risks_facility_type_1y_risk_high_sum              |      0.0106386415 |      10 |  0.03125 |
|      21 | results_zip_code_1y_result_pass w/ conditions_avg |       0.009624423 |      11 | 0.034375 |
|      21 | risks_zip_code_1y_risk_medium_sum                 |      0.0094457436 |      12 |   0.0375 |
|      21 | results_entity_id_3month_result_pass_sum          |       0.009225408 |      13 | 0.040625 |
|      21 | results_entity_id_6month_result_fail_avg          |      0.0091078199 |      14 |  0.04375 |
|      21 | results_entity_id_3month_result_pass_avg          |      0.0088542878 |      15 | 0.046875 |
|      21 | inspections_zip_code_3month_type_canvass_avg      |      0.0087851068 |      16 |     0.05 |
|      21 | risks_entity_id_2y_risk_low_sum                   |         0.0085351 |      17 | 0.053125 |
|      21 | risks_entity_id_2y_risk_low_avg                   |      0.0080073084 |      18 |  0.05625 |
|      21 | results_zip_code_1y_result_fail_sum               |      0.0079038494 |      19 | 0.059375 |
|      21 | results_zip_code_2y_result_fail_avg               |      0.0078180121 |      20 |   0.0625 |
|      21 | results_zip_code_2y_result_pass_sum               |      0.0077150654 |      21 | 0.065625 |
|      21 | inspections_zip_code_2y_type_license_sum          |      0.0076692378 |      22 |  0.06875 |
|      21 | risks_facility_type_3month_risk_high_avg          |      0.0076609972 |      23 | 0.071875 |
|      21 | risks_zip_code_1y_risk_medium_avg                 |      0.0075045563 |      24 |    0.075 |
|      21 | results_zip_code_2y_result_pass w/ conditions_sum |      0.0074907219 |      25 | 0.078125 |
:END:

The list of important features looks reasonable, which increases
our confidence in the results.
