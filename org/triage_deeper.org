#+STARTUP: showeverything
#+STARTUP: nohideblocks
#+STARTUP: indent
#+PROPERTY: header-args:sql :engine postgresql
#+PROPERTY: header-args:sql+ :dbhost 0.0.0.0
#+PROPERTY: header-args:sql+ :dbport 5434
#+PROPERTY: header-args:sql+ :dbuser food_user
#+PROPERTY: header-args:sql+ :dbpassword some_password
#+PROPERTY: header-args:sql+ :database food
#+PROPERTY: header-args:sql+ :results table drawer
#+PROPERTY: header-args:sh   :results output drawer
#+PROPERTY: header-args:ipython   :session food_inspections


* Appendix: A more technical view to =collate=

=collate= helps you with the building of these kind of features. The
previous query is splitted in two parts in =collate=: First, the
column, the filter and the aggregate operation to be Applied
 are part of a =Categorical= object. 

#+BEGIN_SRC ipython
from  triage.component.collate import Categorical

risks = Categorical("risk", # the column
                    ["high", "medium", "low"], # compare to, i.e. 'risk = high', 'risk=low', etc
                    "avg", # aggregation function
                    {'coltype':'categorical', 'all': {'type': 'zero'}} # imputation rules
)


#+END_SRC

#+RESULTS:
:RESULTS:

:END:


Note also that =collate= handles the imputation: we specified the
imputation strategy for how to handle 
the null values in the resulting fields, in this example we use the
=zero= value (if the aggregation is =NULL= impute with =0=).

The second part is the actual aggregator, called
=SparcetimeAggregation=, this object handles the /context/ of the aggregation.


#+BEGIN_SRC ipython
import sqlalchemy
from triage.component.collate import  SpacetimeAggregation

# We need a connection to the data base
db_url = f"postgresql://food_user:some_password@0.0.0.0:5434/food"
engine = sqlalchemy.create_engine(db_url, client_encoding='utf8')

db_connection = engine.connect()

st = SpacetimeAggregation([risks], # The Categorical object
                          from_obj='triage.test', # FROM
                          groups=['entity_id','zip_code'],  # GROUP BY
                          dates=["2014-10-06",
                                 "2014-10-08",
                                 "2015-01-12",
                                 "2015-10-20",
                                 "2016-10-17"], # AS OF DATES, This comes from Timechop, are used as 'WHERE date = ...'
                          intervals={"entity_id": ["1 year"], "zip_code": ["1 year"]}, # This will be used as the intervals in the past of the AS OF DATE
                          date_column="date", # Which is the name of the date column?
                          state_table='triage.active_facilities_9581', # State table name
                          state_group='entity_id', # Which is the column that identifies the entity
                          output_date_column='date',
                          schema='triage', # In which schema do you want to store the results?
                          prefix='risks'
)

#+END_SRC

#+RESULTS:
:RESULTS:

:END:

The =SpacetimeAggregation= object is in charge of create the
agregations, another way of see it, is that it encapsulates the =From=
 part of the query (=from_obj=) as well as the =GROUP BY= columns (=groups=).

In the example above it will create features based on individual
restaurants (using =entity_id=) but also /contextual/ features related
to information about the zip code (=zip_code=) in which the facility is
operating.

The state table (=state_table=) specified here should contain the
comprehensive set of facilities and dates for which output should be
generated for them, regardless if they exist in the =from_obj=.

The attribute =intervals= specifies the date range partitioning for the
feature: it will create the aggregation over the past =1 year= for the
grouping given by the =entity_id= nad for the =zip_code=, and
additionally  will give an extra grouping statistic of two months for
the =zip_code=.

Before execute the queries, you could actually look them using the following

#+BEGIN_SRC ipython
import utils

utils.show_features_queries(st)
#+END_SRC


This will execute queries as the following for the group tables (like =test_risks_zip_code=):

#+BEGIN_EXAMPLE sql
...

SELECT zip_code, '2014-10-08'::date AS date,
avg((risk = 'high')::INT) FILTER (WHERE date >= '2014-10-08'::date - interval '1 year') AS "test_risks_zip_code_1 year_risk_high_avg",
avg((risk = 'medium')::INT) FILTER (WHERE date >= '2014-10-08'::date - interval '1 year') AS "test_risks_zip_code_1 year_risk_medium_avg",
avg((risk = 'low')::INT) FILTER (WHERE date >= '2014-10-08'::date - interval '1 year') AS "test_risks_zip_code_1 year_risk_low_avg"
FROM triage.test
WHERE date < '2014-10-08'AND date >= '2014-10-08'::date - greatest(interval '1 year') GROUP BY zip_code

...
#+END_EXAMPLE

and the next query for the =risks_aggregation= table:

#+BEGIN_EXAMPLE sql
CREATE TABLE "triage"."risks_aggregation" AS (
SELECT * FROM (
SELECT entity_id, zip_code, '2014-10-06'::date AS date
FROM triage.test
WHERE date < '2014-10-06'AND date >= '2014-10-06'::date - greatest(interval '1 year') GROUP BY entity_id, zip_code
UNION ALL
SELECT entity_id, zip_code, '2014-10-08'::date AS date
FROM triage.test
WHERE date < '2014-10-08'AND date >= '2014-10-08'::date - greatest(interval '1 year') GROUP BY entity_id, zip_code
UNION ALL
SELECT entity_id, zip_code, '2015-01-12'::date AS date
FROM triage.test
WHERE date < '2015-01-12'AND date >= '2015-01-12'::date - greatest(interval '1 year') GROUP BY entity_id, zip_code
UNION ALL
SELECT entity_id, zip_code, '2015-10-20'::date AS date
FROM triage.test
WHERE date < '2015-10-20'AND date >= '2015-10-20'::date - greatest(interval '1 year') GROUP BY entity_id, zip_code
UNION ALL
SELECT entity_id, zip_code, '2016-10-17'::date AS date
FROM triage.test
WHERE date < '2016-10-17'AND date >= '2016-10-17'::date - greatest(interval '1 year') GROUP BY entity_id, zip_code
) t1
LEFT JOIN "triage"."test_risks_entity_id" USING (entity_id, date) LEFT JOIN "triage"."test_risks_zip_code" USING (zip_code, date));
#+END_EXAMPLE

You can create the features tables executing the following:

#+BEGIN_SRC ipython
st.execute(db_connection) # with a SQLAlchemy engine object
#+END_SRC


#+RESULTS:

This will create 3 tables (One for the =entity_id=, one for =zip_code=
and one for the combination: =entity_id + zip_code=) and one extra
table for the imputated values.

The names of the generated tables are constructed as follows:

#+BEGIN_EXAMPLE
schema.prefix_{group, aggregation}
#+END_EXAMPLE

Inside each of those new tables, the column name will follow this
pattern:

#+BEGIN_EXAMPLE
prefix_group_interval_categorical_operation
#+END_EXAMPLE

For example the tables inside the triage schema are:

#+BEGIN_SRC sql
\dt triage.risks*
#+END_SRC

#+RESULTS:
:RESULTS:
| List of relations |                         |       |          |
|-------------------+-------------------------+-------+----------|
| Schema            | Name                    | Type  | Owner    |
| triage            | risks_aggregation        | table | food_user |
| triage            | risks_aggregation_imputed | table | food_user |
| triage            | risks_entity_id           | table | food_user |
| triage            | risks_zip_code            | table | food_user |
:END:

And inside =test_risk_aggregation= the columns are:

#+BEGIN_SRC sql
\d triage.risks_aggregation
#+END_SRC

#+RESULTS:
:RESULTS:
| Table "triage.risks_aggregation"  |                   |           |
|----------------------------------+-------------------+-----------|
| Column                           | Type              | Modifiers |
| zip_code                          | character varying |           |
| date                             | date              |           |
| entity_id                         | bigint            |           |
| risks_entity_id_1 year_risk_high_avg   | numeric           |           |
| risks_entity_id_1 year_risk_medium_avg | numeric           |           |
| risks_entity_id_1 year_risk_low_avg    | numeric           |           |
| risks_zip_code_1 year_risk_high_avg    | numeric           |           |
| risks_zip_code_1 year_risk_medium_avg  | numeric           |           |
| risks_zip_code_1 year_risk_low_avg     | numeric           |           |
:END:


The =triage.risks_zip_code= table
have two feature columns for every zip code in our table =triage.test=,
looking at the total and average number of complaints in that
=zip_code= over the year prior and 2 months prior to the date in the =date= column.


#+BEGIN_SRC sql
select * from triage.risks_zip_code  order by date limit 5;
#+END_SRC

#+RESULTS:
:RESULTS:
| zip_code |       date | risks_zip_code_1 year_risk_high_avg | risks_zip_code_1 year_risk_medium_avg | risks_zip_code_1 year_risk_low_avg |
|---------+------------+-------------------------------+---------------------------------+------------------------------|
|   60621 | 2014-10-06 |        0.00000000000000000000 |          1.00000000000000000000 |       0.00000000000000000000 |
|   60621 | 2014-10-08 |        0.00000000000000000000 |          1.00000000000000000000 |       0.00000000000000000000 |
|   60621 | 2015-01-12 |        0.00000000000000000000 |          1.00000000000000000000 |       0.00000000000000000000 |
|   60621 | 2015-10-20 |        0.00000000000000000000 |          1.00000000000000000000 |       0.00000000000000000000 |
|   60621 | 2016-10-17 |        0.00000000000000000000 |          1.00000000000000000000 |       0.00000000000000000000 |
:END:

The table =triage.risks_entity_id= contains two feature columns for each
license that describe the total number of complaints
the past one year.

#+BEGIN_SRC sql
select * from triage.risks_entity_id  order by date limit 5;
#+END_SRC

#+RESULTS:
:RESULTS:
| entity_id |       date | risks_entity_id_1 year_risk_high_avg | risks_entity_id_1 year_risk_medium_avg | risks_entity_id_1 year_risk_low_avg |
|----------+------------+--------------------------------+----------------------------------+-------------------------------|
|     9547 | 2014-10-06 |         0.00000000000000000000 |           1.00000000000000000000 |        0.00000000000000000000 |
|     9547 | 2014-10-08 |         0.00000000000000000000 |           1.00000000000000000000 |        0.00000000000000000000 |
|     9547 | 2015-01-12 |         0.00000000000000000000 |           1.00000000000000000000 |        0.00000000000000000000 |
|     9547 | 2015-10-20 |         0.00000000000000000000 |           1.00000000000000000000 |        0.00000000000000000000 |
|     9547 | 2016-10-17 |         0.00000000000000000000 |           1.00000000000000000000 |        0.00000000000000000000 |
:END:

The =triage.risk_aggregation= table joins these results together to make
it easier to look at both zip_code and facility-level effects
for any given facility.

#+BEGIN_SRC sql
select * from triage.risks_aggregation order by date limit 5;
#+END_SRC

#+RESULTS:
:RESULTS:
| zip_code |       date | entity_id | risks_entity_id_1 year_risk_high_avg | risks_entity_id_1 year_risk_medium_avg | risks_entity_id_1 year_risk_low_avg | risks_zip_code_1 year_risk_high_avg | risks_zip_code_1 year_risk_medium_avg | risks_zip_code_1 year_risk_low_avg |
|---------+------------+----------+--------------------------------+----------------------------------+-------------------------------+-------------------------------+---------------------------------+------------------------------|
|   60621 | 2014-10-06 |     9547 |         0.00000000000000000000 |           1.00000000000000000000 |        0.00000000000000000000 |        0.00000000000000000000 |          1.00000000000000000000 |       0.00000000000000000000 |
|   60621 | 2014-10-08 |     9547 |         0.00000000000000000000 |           1.00000000000000000000 |        0.00000000000000000000 |        0.00000000000000000000 |          1.00000000000000000000 |       0.00000000000000000000 |
|   60621 | 2015-01-12 |     9547 |         0.00000000000000000000 |           1.00000000000000000000 |        0.00000000000000000000 |        0.00000000000000000000 |          1.00000000000000000000 |       0.00000000000000000000 |
|   60621 | 2015-10-20 |     9547 |         0.00000000000000000000 |           1.00000000000000000000 |        0.00000000000000000000 |        0.00000000000000000000 |          1.00000000000000000000 |       0.00000000000000000000 |
|   60621 | 2016-10-17 |     9547 |         0.00000000000000000000 |           1.00000000000000000000 |        0.00000000000000000000 |        0.00000000000000000000 |          1.00000000000000000000 |       0.00000000000000000000 |
:END:


Finally, the =triage.risks_aggregated_imputed= table fills in null values using the
imputation rules specified in the =Categorical= constructor.

Let's try with =inspection_type=. We want the total of =canvass= and
=complaint= inspections.

#+BEGIN_SRC python

inspection_types = Categorical("inspection_type", # the column
                    ["canvass", "complaint"], # compare to, i.e. 'inspection_type = canvass', etc.
                    "sum", # aggregation function
                    {'coltype':'categorical', 'all': {'type': 'zero'}} # imputation rules
)

st = SpacetimeAggregation([inspection_types], # The Categorical object
                          from_obj='triage.test', # FROM
                          groups=['entity_id','zip_code'],  # GROUP BY
                          dates=["2014-10-06",
                                 "2014-10-08",
                                 "2015-01-12",
                                 "2015-10-20",
                                 "2016-10-17"], # AS OF DATES, This comes from Timechop, are used as 'WHERE date = ...'
                          intervals={"entity_id": ["1y"], "zip_code": ["1y"]}, # This will be used as the intervals in the past of the AS OF DATE
                          date_column="date", # Which is the name of the date column?
                          state_table='triage.active_facilities', # State table name
                          state_group='entity_id', # Which is the column that identifies the entity
                          output_date_column='date',
                          schema='triage', # In which schema do you want to store the results?
                          prefix='inspection_type'
)

st.execute(db_connection)
#+END_SRC

#+RESULTS:
:RESULTS:
:END:

This will create, as you probably guessed, four new tables:
=inspection_type_{entity_id, zip_code, aggregation, aggregation_imputed}=


Or you can mix the two in one step:

#+BEGIN_SRC python
st = SpacetimeAggregation([risks,inspection_types], # The Categorical object
                          from_obj='triage.test', # FROM
                          groups=['entity_id','zip_code'],  # GROUP BY
                          dates=["2014-10-06",
                                 "2014-10-08",
                                 "2015-01-12",
                                 "2015-10-20",
                                 "2016-10-17"], # AS OF DATES, This comes from Timechop, are used as 'WHERE date = ...'
                          intervals={"entity_id": ["1y"], "zip_code": ["1y"]}, # This will be used as the intervals in the past of the AS OF DATE
                          date_column="date", # Which is the name of the date column?
                          state_table='triage.active_facilities', # State table name
                          state_group='entity_id', # Which is the column that identifies the entity
                          output_date_column='date',
                          schema='triage', # In which schema do you want to store the results?
                          prefix='both'
)

st.execute(db_connection)
#+END_SRC

#+RESULTS:
:RESULTS:
:END:


Checking the columns inside =triage.both_aggregation= , you will note
that all the previous columns are there (except for the prefix):

#+BEGIN_SRC sql
\d triage.both_aggregation
#+END_SRC

#+RESULTS:
:RESULTS:
| Table "triage.both_aggregation"           |                   |           |
|------------------------------------------+-------------------+-----------|
| Column                                   | Type              | Modifiers |
| zip_code                                  | character varying |           |
| date                                     | date              |           |
| entity_id                                 | bigint            |           |
| both_entity_id_1y_risk_high_avg                | numeric           |           |
| both_entity_id_1y_risk_medium_avg              | numeric           |           |
| both_entity_id_1y_risk_low_avg                 | numeric           |           |
| both_entity_id_1y_inspection_type_canvass_sum   | bigint            |           |
| both_entity_id_1y_inspection_type_complaint_sum | bigint            |           |
| both_zip_code_1y_risk_high_avg                 | numeric           |           |
| both_zip_code_1y_risk_medium_avg               | numeric           |           |
| both_zip_code_1y_risk_low_avg                  | numeric           |           |
| both_zip_code_1y_inspection_type_canvass_sum    | bigint            |           |
| both_zip_code_1y_inspection_type_complaint_sum  | bigint            |           |
:END:


Obviously you could want to create more complicated variables, for
example, we have a =json= column in our =semantic.events= table, as well
as a geographical column: =location=. Let's do create some features
using those.


** Add number of violations by severity

Our =semantic.events= has a =json= column called =violations=. We will like
to have an idea of how many types of violations were inspected or at
least their severity. One way of do that is shown in the next =SQL= code:


#+BEGIN_SRC sql
select inspection, entity_id, zip_code, array_agg(obj ->> 'severity'),
count(*) filter (where obj ->> 'severity' = 'critical') as critical_violations,
count(*) filter (where obj ->> 'severity' = 'serious') as serious_violations,
count(*) filter (where obj ->> 'severity' = 'minor') as low_violations
from
(select inspection, entity_id, zip_code, jsonb_array_elements(violations::jsonb) as obj from triage.test)
as t1
group by inspection, entity_id, zip_code
limit 5
#+END_SRC

#+RESULTS:
:RESULTS:
| inspection | entity_id | zip_code | array_agg                        | critical_violations | serious_violations | low_violations |
|------------+----------+---------+---------------------------------+--------------------+-------------------+---------------|
|    1076221 |     9547 |   60621 | {minor,minor,minor}             |                  0 |                 0 |             3 |
|    1150580 |     9547 |   60621 | {minor,minor,minor,minor,minor} |                  0 |                 0 |             5 |
|    1150633 |     9547 |   60621 | {minor,minor,minor,minor}       |                  0 |                 0 |             4 |
|    1150878 |     9547 |   60621 | {minor,minor,minor,minor}       |                  0 |                 0 |             4 |
|    1150936 |     9547 |   60621 | {minor,minor,minor}             |                  0 |                 0 |             3 |
:END:

Basically, this code gives us the number of violations inspected by
severity. How about to get the total and proportion of violations in a
facility in the previous year and the average and standard deviation
for the zip code zone?  Note than in this case the variable is not
*categorical*, is a numeric one, fortunately =collate= also provides
support for numerical variables: the =Aggregate= object

#+BEGIN_SRC python
from  triage.component.collate import Aggregate


violations_sql = """
(
select inspection, entity_id, zip_code, date,
count(*) filter (where obj ->> 'severity' = 'critical') as critical_violations,
count(*) filter (where obj ->> 'severity' = 'serious') as serious_violations,
count(*) filter (where obj ->> 'severity' = 'minor') as low_violations
from
(select inspection, entity_id, zip_code, date, jsonb_array_elements(violations::jsonb) as obj from triage.test)
as t1
group by inspection, entity_id, zip_code, date
) as t
"""

critical_violations = Aggregate({'critical': 'critical_violations'}, ['sum', 'avg', 'stddev'], {'coltype':'aggregate', 'all': {'type': 'mean'}})
serious_violations = Aggregate({'serious': 'serious_violations'}, ['sum', 'avg', 'stddev'], {'coltype':'aggregate', 'all': {'type': 'mean'}})
low_violations = Aggregate({'low': 'low_violations'}, ['sum', 'avg', 'stddev'], {'coltype':'aggregate', 'all': {'type': 'mean'}})

st = SpacetimeAggregation([critical_violations, serious_violations, low_violations], # The Categorical object
                          from_obj=violations_sql, # FROM
                          groups=['entity_id','zip_code', 'inspection'],  # GROUP BY
                          dates=["2014-10-06",
                                 "2014-10-08",
                                 "2015-01-12",
                                 "2015-10-20",
                                 "2016-10-17"], # AS OF DATES, This comes from Timechop, are used as 'WHERE date = ...'
                          intervals={"entity_id": ["1y"], "zip_code": ["1y"], "inspection": ["0d"]}, # This will be used as the intervals in the past of the AS OF DATE
                          date_column="date", # Which is the name of the date column?
                          state_table='triage.active_facilities', # State table name
                          state_group='entity_id', # Which is the column that identifies the entity
                          output_date_column='date',
                          schema='triage', # In which schema do you want to store the results?
                          prefix='violations'
)

st.execute(db_connection)

#+END_SRC

We can inspect the generated =SQL=:


#+BEGIN_EXAMPLE sql
...

SELECT entity_id, '2014-10-06'::date AS date,
sum(critical_violations) FILTER (WHERE date >= '2014-10-06'::date - interval '1y') AS violations_entity_id_1y_critical_sum,
avg(critical_violations) FILTER (WHERE date >= '2014-10-06'::date - interval '1y') AS violations_entity_id_1y_critical_avg,
stddev(critical_violations) FILTER (WHERE date >= '2014-10-06'::date - interval '1y') AS violations_entity_id_1y_critical_stddev,
sum(serious_violations) FILTER (WHERE date >= '2014-10-06'::date - interval '1y') AS violations_entity_id_1y_serious_sum,
avg(serious_violations) FILTER (WHERE date >= '2014-10-06'::date - interval '1y') AS violations_entity_id_1y_serious_avg,
stddev(serious_violations) FILTER (WHERE date >= '2014-10-06'::date - interval '1y') AS violations_entity_id_1y_serious_stddev,
sum(low_violations) FILTER (WHERE date >= '2014-10-06'::date - interval '1y') AS violations_entity_id_1y_low_sum,
 avg(low_violations) FILTER (WHERE date >= '2014-10-06'::date - interval '1y') AS violations_entity_id_1y_low_avg,
stddev(low_violations) FILTER (WHERE date >= '2014-10-06'::date - interval '1y') AS violations_entity_id_1y_low_stddev
FROM
(
select inspection, entity_id, zip_code, date,
count(*) filter (where obj ->> 'severity' = 'critical') as critical_violations,
count(*) filter (where obj ->> 'severity' = 'serious') as serious_violations,
count(*) filter (where obj ->> 'severity' = 'minor') as low_violations
from
(select inspection, entity_id, zip_code, date, jsonb_array_elements(violations::jsonb) as obj from triage.test)
as t1
group by inspection, entity_id, zip_code, date
) as t
WHERE date < '2014-10-06'AND date >= '2014-10-06'::date - greatest(interval '1y') GROUP BY entity_id

...

#+END_EXAMPLE

You should learn this: it is possible to pass any PostgreSQL =table=
object to =collate= (and henceforth to =triage=), even those which result from a query.


** Add number of facilities by type in a radius: 1km

Another possible interesting feature is related to the spatial
surroundings of the facility. Is the facility near of schools or
day cares? or Is the facility located in zones with a lot of people
passing by?

In this feature the spatial aggregation is not some socio-political entity
as the zip code or the city limits, but the location
of the facility and the radius that defines the "neighborhood" of the facility.

The following query returns the number of facilities in a radius of 1
km around the facility in our example (=entity_id= 9547).

#+BEGIN_SRC sql
with inspected_facilities as (
    select distinct on (entity_id, location, facility_type) *
    from triage.active_facilities
),

facilities_nearby as (
   select
   a.entity_id, a.location, a.facility_type,
   b.entity_id as other_entity_id,
   b.facility_type as other_facility_type
   from inspected_facilities as a,
   lateral (
       select entity_id, facility_type
       from semantic.entities
       where ST_DWithin(location::geography, a.location::geography, 1000)  -- In meteres, i.e. 1000 m = 1 km
       and entity_id <> a.entity_id
   ) as b
)

select
entity_id,
other_facility_type, count(*) as total
from facilities_nearby
group by
entity_id, other_facility_type
#+END_SRC

#+RESULTS:
:RESULTS:
| entity_id | other_facility_type               | total |
|----------+---------------------------------+-------|
|     9547 | school                          |    11 |
|     9547 | convenience store               |     2 |
|     9547 | furniture store                 |     1 |
|     9547 | long term care                  |     6 |
|     9547 | wholesale                       |     1 |
|     9547 | daycare (2 - 6 years)           |     3 |
|     9547 | golden diner                    |     1 |
|     9547 | private school                  |     2 |
|     9547 | grocery store                   |    40 |
|     9547 | restaurant                      |    37 |
|     9547 | children's services facility    |     1 |
|     9547 | daycare above and under 2 years |     4 |
|     9547 | gas station                     |     1 |
|     9547 | bakery                          |     1 |
|     9547 | unknown                         |    23 |
:END:

#+BEGIN_SRC python
facilities_nearby_sql = """
(
with inspected_facilities as (
    select distinct on (entity_id, location, facility_type) *
    from triage.active_facilities
),

facilities_nearby as (
   select
   a.entity_id, a.location, a.facility_type,
   b.entity_id as other_entity_id,
   b.facility_type as other_facility_type
   from inspected_facilities as a,
   lateral (
       select entity_id, facility_type
       from semantic.entities
       where ST_DWithin(location::geography, a.location::geography, 1000)  -- In meters i.e. 1000 m = 1 km
       and entity_id <> a.entity_id
   ) as b
)

select
entity_id,
other_facility_type, count(*) as total
from facilities_nearby
group by
entity_id, other_facility_type
)
"""
# TODO: Create the correct aggregate Do I need to pivot the table?
facilities = Aggregate({})

st = SpacetimeAggregation([critical_violations, serious_violations, low_violations], # The Categorical object
                          from_obj=violations_sql, # FROM
                          groups=['entity_id','zip_code', 'inspection'],  # GROUP BY
                          dates=["2014-10-06",
                                 "2014-10-08",
                                 "2015-01-12",
                                 "2015-10-20",
                                 "2016-10-17"], # AS OF DATES, This comes from Timechop, are used as 'WHERE date = ...'
                          intervals={"entity_id": ["1y"], "zip_code": ["1y"], "inspection": ["0d"]}, # This will be used as the intervals in the past of the AS OF DATE
                          date_column="date", # Which is the name of the date column?
                          state_table='triage.all_facilities', # State table name
                          state_group='entity_id', # Which is the column that identifies the entity
                          output_date_column='date',
                          schema='triage', # In which schema do you want to store the results?
                          prefix='violations'
)

st.execute(db_connection)

#+END_SRC


** Add number of inspections by type in a radius and in an interval

Based in the previous example, we can modify a little the question an
ask: How many events happened nearby?

#+BEGIN_SRC sql
with inspected_facilities as (
    select distinct on (entity_id, location, facility_type) *
    from triage.active_facilities
),

inspections_nearby as (
   select
   a.entity_id,
   b.inspection, b.type, b.result, b.entity_id as other_entity_id,
   b.facility_type as other_facility_type, b.date
   from inspected_facilities as a,
   lateral (
       select inspection, type, result, entity_id, facility_type, date
       from semantic.events
       where ST_DWithin(location::geography, a.location::geography, 1000)  -- In meteres, i.e. 1000 m = 1 km
       and entity_id <> a.entity_id
   ) as b
)

select * from inspections_nearby limit 10;

-- select
-- entity_id, inspection, type as inpection
-- other_facility_type, count(*) as total
-- from inspections_nearby
-- group by
-- entity_id, other_facility_type
#+END_SRC

#+RESULTS:
:RESULTS:
| entity_id | inspection | type      | result             | other_entity_id | other_facility_type |       date |
|----------+------------+-----------+--------------------+---------------+-------------------+------------|
|     9547 |     920206 | complaint | pass               |          1811 | grocery store     | 2012-02-24 |
|     9547 |     545345 | complaint | fail               |          1811 | grocery store     | 2011-05-11 |
|     9547 |     670596 | complaint | fail               |          1811 | grocery store     | 2012-02-17 |
|     9547 |     545365 | complaint | pass               |          1811 | grocery store     | 2011-05-19 |
|     9547 |    1955217 | canvass   | pass               |          1811 | grocery store     | 2016-09-08 |
|     9547 |    2081502 | complaint | pass               |          1811 | grocery store     | 2017-09-05 |
|     9547 |     545517 | complaint | pass               |          1811 | grocery store     | 2011-09-08 |
|     9547 |    1386161 | complaint | fail               |          1811 | grocery store     | 2015-05-27 |
|     9547 |    1386181 | complaint | pass w/ conditions |          1811 | grocery store     | 2015-06-04 |
|     9547 |     531602 | canvass   | pass               |          2111 | long term care    | 2011-10-06 |
:END:




QUESTION: Is this the correct SQL? (at least is fast)
IDEA: We could precalculate the distances? And from that filter by date?

#+BEGIN_SRC sql
with inspected_same_day as (
 select
   a.inspection, a.entity_id, a.location, a.facility_type, a.date,
   b.inspection as other_inspection, b.facility_type as other_facility_type, b.location as other_location
   from triage.test as a,
   lateral (
      select inspection, entity_id, location, facility_type, date
      from semantic.events
      where inspection <> a.inspection
      and date = a.date
   ) as b
),

inspections_nearby as (
   select
   inspection, entity_id, location, facility_type,
   other_facility_type, date
   from inspected_same_day
   where
       ST_DWithin(location::geography, other_location::geography, 1000)
)

select
inspection, entity_id, location, facility_type,
other_facility_type, date, count(*)
from inspections_nearby
group by
inspection, entity_id, location, facility_type, other_facility_type, date
limit 10
#+END_SRC

#+RESULTS:
:RESULTS:
| inspection | entity_id | location                                           | facility_type | other_facility_type |       date | count |
|------------+----------+----------------------------------------------------+--------------+-------------------+------------+-------|
|    1150878 |     9547 | 0101000020E6100000BF53EAB21DE855C0EC6799AE73E24440 | restaurant   | restaurant        | 2012-10-26 |     1 |
|    1150878 |     9547 | 0101000020E6100000BF53EAB21DE855C0EC6799AE73E24440 | restaurant   | school            | 2012-10-26 |     1 |
|    1335951 |     9547 | 0101000020E6100000BF53EAB21DE855C0EC6799AE73E24440 | restaurant   | restaurant        | 2013-06-06 |     1 |
|    1335951 |     9547 | 0101000020E6100000BF53EAB21DE855C0EC6799AE73E24440 | restaurant   | school            | 2013-06-06 |     2 |
|    1353423 |     9547 | 0101000020E6100000BF53EAB21DE855C0EC6799AE73E24440 | restaurant   | grocery store     | 2013-07-01 |     1 |
|    1360667 |     9547 | 0101000020E6100000BF53EAB21DE855C0EC6799AE73E24440 | restaurant   | grocery store     | 2013-09-04 |     1 |
|    1591635 |     9547 | 0101000020E6100000BF53EAB21DE855C0EC6799AE73E24440 | restaurant   | grocery store     | 2015-12-14 |     1 |
|    1657238 |     9547 | 0101000020E6100000BF53EAB21DE855C0EC6799AE73E24440 | restaurant   | restaurant        | 2016-02-25 |     1 |
|     401429 |     9547 | 0101000020E6100000BF53EAB21DE855C0EC6799AE73E24440 | restaurant   | gas station       | 2011-01-13 |     1 |
|     401429 |     9547 | 0101000020E6100000BF53EAB21DE855C0EC6799AE73E24440 | restaurant   | wholesale         | 2011-01-13 |     1 |
:END:

#+BEGIN_SRC python
inspections_nearby_sql = """
with inspections_nearby as (
with inspected_same_day as (
 select
   a.inspection, a.entity_id, a.location, a.facility_type, a.date,
   b.inspection as other_inspection, b.facility_type as other_facility_type, b.location as other_location
   from triage.test as a,
   lateral (
      select inspection, entity_id, location, facility_type, date
      from semantic.events
      where inspection <> a.inspection
      and date = a.date
   ) as c
),

inspections_nearby as (
   select
   inspection, entity_id, location, facility_type,
   other_facility_type, date
   from inspected_same_day
   where
       ST_DWithin(location::geography, other_location::geography, 1000)
)

select
inspection, entity_id, location, facility_type,
other_facility_type, date, count(*)
from inspections_nearby
group by
inspection, entity_id, location, facility_type, other_facility_type, date
) as b
"""

# TODO: Create the correct aggregate Do I need to pivot the table?
facilities = Aggregate({})

st = SpacetimeAggregation([critical_violations, serious_violations, low_violations], # The Categorical object
                          from_obj=violations_sql, # FROM
                          groups=['entity_id','zip_code', 'inspection'],  # GROUP BY
                          dates=["2014-10-06",
                                 "2014-10-08",
                                 "2015-01-12",
                                 "2015-10-20",
                                 "2016-10-17"], # AS OF DATES, This comes from Timechop, are used as 'WHERE date = ...'
                          intervals={"entity_id": ["1y"], "zip_code": ["1y"], "inspection": ["0d"]}, # This will be used as the intervals in the past of the AS OF DATE
                          date_column="date", # Which is the name of the date column?
                          state_table='triage.all_facilities', # State table name
                          state_group='entity_id', # Which is the column that identifies the entity
                          output_date_column='date',
                          schema='triage', # In which schema do you want to store the results?
                          prefix='violations'
)

st.execute(db_connection)

#+END_SRC




==================


You can use the docker image provider for getting an image about the
temporal blocks configuration, for example [[file:src/inspections_test.yaml][inspections_test.yaml]] has the following temporal structure:


#+BEGIN_SRC sh
./tutorial.sh triage --config_file inspections_test.yaml show-temporal-blocks
#+END_SRC

#+RESULTS:
:RESULTS:
Creating experiment object
Experiment loaded
Generating temporal blocks image
Image stored in:
/code/inspections_test.png
:END:

[[./src/inspections_test.png]]


** The =Experiment= object

Everything revolves around the =experiment= object. You can validate
an =experiment= as follows:

#+BEGIN_SRC sh
./tutorial.sh triage --config_file /code/inspections_test.yaml validate
#+END_SRC

#+RESULTS:
:RESULTS:
Creating experiment object
Experiment loaded
Validating experiment's configuration
:END:

If you are ready for run it, you could start your experiment with:

#+BEGIN_SRC sh
./tutorial.sh triage --config_file /code/inspections_test.yaml run
#+END_SRC


TODO: What is the meaning of the average on collate?
