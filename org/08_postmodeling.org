#+STARTUP: showeverything
#+STARTUP: nohideblocks
#+STARTUP: indent
#+STARTUP: align
#+STARTUP: inlineimages
#+STARTUP: latexpreview
#+PROPERTY: header-args:sql :engine postgresql
#+PROPERTY: header-args:sql+ :dbhost 0.0.0.0
#+PROPERTY: header-args:sql+ :dbport 5434
#+PROPERTY: header-args:sql+ :dbuser food_user
#+PROPERTY: header-args:sql+ :dbpassword some_password
#+PROPERTY: header-args:sql+ :database food
#+PROPERTY: header-args:sql+ :results table drawer
#+PROPERTY: header-args:sql+ :exports both
#+PROPERTY: header-args:sql+ :eval no-export
#+PROPERTY: header-args:sql+ :cmdline -q
#+PROPERTY: header-args:sh  :results verbatim org
#+PROPERTY: header-args:sh+ :prologue exec 2>&1 :epilogue :
#+PROPERTY: header-args:ipython   :session food_inspections
#+PROPERTY: header-args:ipython+ :results raw drawer
#+OPTIONS: broken-links:mark
#+OPTIONS: tasks:todo
#+OPTIONS: LaTeX:t

* WORKING Audition: So many models, how can I choose the best one?

Now, you have *36* /model groups/ related to EIS. Which one is best? What do you mean by
/best/? Which should you choose to use? This is not as easy as it
sounds, due to several factors:

- You can try to pick the best using a metric
  specified in the config file (=precision@= and =recall@=),
  but at what point of time? Maybe different model groups are best
  at different prediction times.
- You can just use the one that performs best on the last test set.
- You can value a model group that provides consistent results over time.
  It might not be the best on any test set, but you can feel more
  confident that it will continue to perform similarly.
- If there are several model groups that perform similarly and
  their lists are more or less similar, maybe it doesn't really
  matter which you pick.

The answers to questions like these may not be obvious up front.

Triage provides this functionality in =audition= and in
=postmodel=. At the moment of this writing, these two modules require
more interaction (i.e. they aren't integrated with the /configuration
file/).

Audition is a tool for picking the best trained classifiers from a
predictive analytics experiment.  Audition introduces
a structured, semi-automated way of filtering models based on what you
consider important.

=Audition= formalizes this idea through /selection rules/ that take in
the data up to a given point in time, apply some rule to choose a
model group, and then evaluate the performance (*regret*) of the chosen
model group in the subsequent time window.

=Audition= predefines 7 rules:

1. =best_current_value= :: Pick the model group with the best current metric Value.
2. =best_average_value= :: Pick the model with the highest average metric value so far.
3. =lowest_metric_variance= :: Pick the model with the lowest metric variance so far.
4. =most_frequent_best_dist= :: Pick the model that is most frequently
     within =dist_from_best_case= from the best-performing model group
     across test sets so far.
5. =best_average_two_metrics= :: Pick the model with the highest
     average combined value to date of two metrics weighted together
     using =metric1_weight=.
6. =best_avg_var_penalized= :: Pick the model with the highest average
     metric value so far, penalized for relative variance ss:
     =avg_value - (stdev_penalty) * (stdev - min_stdev)= where
     =min_stdev= is the minimum standard deviation of the metric
     across all model groups
7.  =best_avg_recency_weight= :: Pick the model with the highest
     average metric value so far, penalized for relative variance as:
     =avg_value - (stdev_penalty) * (stdev - min_stdev)= where
     =min_stdev= is the minimum standard deviation of the metric
     across all  model groups

We included a simple configuration file with some rules:

#+BEGIN_SRC yaml :tangle ../triage/audition_config.yaml
# CHOOSE MODEL GROUPS
model_groups:
    query: |
        select distinct(model_group_id)
        from model_metadata.model_groups
        where model_config ->> 'experiment_type' ~ 'eis'
# CHOOSE TIMESTAMPS/TRAIN END TIMES
time_stamps:
    query: |
        select distinct train_end_time
        from model_metadata.models
        where model_group_id in ({})
        and extract(day from train_end_time) in (1)
        and train_end_time >= '2015-01-01'
# FILTER
filter:
    metric: 'precision@' # metric of interest
    parameter: '10_pct' # parameter of interest
    max_from_best: 1.0 # The maximum value that the given metric can be worse than the best model for a given train end time.
    threshold_value: 0.0 # The worst absolute value that the given metric should be.
    distance_table: 'distance_table' # name of the distance table
    models_table: 'models' # name of the models table

# RULES
rules:
    -
        shared_parameters:
            -
                metric: 'precision@'
                parameter: '10_pct'

        selection_rules:
            -
                name: 'best_current_value' # Pick the model group with the best current metric value
                n: 3
            -
                name: 'best_average_value' # Pick the model with the highest average metric value
                n: 3
            -
                name: 'lowest_metric_variance' # Pick the model with the lowest metric variance
                n: 3
            -
                name: 'most_frequent_best_dist' # Pick the model that is most frequently within `dist_from_best_case`
                dist_from_best_case: [0.05]
                n: 3

#+END_SRC

=Audition= will have each rule give you the best $n$ model-group IDs
based on the metric and parameter following that rule for the most
recent time period (in all the rules shown $n$ = 1).

We can run the simulation of the rules againts the experiment as:

#+BEGIN_SRC sh :dir /docker:root@tutorial_bastion:/triage :exports code :results none
triage --tb audition --directory audition/
#+END_SRC



=Audition= will create several plots that will help you to sort out
which is the /best[fn:1]/ model group to use (like in a production setting or
just to generate your list).

Most of the time, =audition= should be used iteratively, the result of
each iteration will be a reduced set of models groups and a best rule
for selecting model groups.

** Filtering model groups

=Audition= will generate two plots that are meant to be used together:
/model performance over time/ and /distance from best/.

#+CAPTION: Model group performance over time. In this case the metric show is =precision@10%=. The black dashed line represents the (theoretical) system's performance if we select the best performant model in a every evaluation date. The colored lines represents different model groups. All the model groups that share an algorithm will be colored the same.
#+LABEL: fig:performance-over-time
[[file:~/projects/dsapp/dirtyduck/triage/audition/metric_over_time_precision@10_pct.png]]


#+CAPTION: Proportion of /models/ in a /model group/ that are separated from the best model in an specific evaluation time. The distance is measured in percentual points, i.e. How much less precision at 10 percent of the population compared to the best model in that date.
#+LABEL: fig:distance-from-best
[[file:~/projects/dsapp/dirtyduck/triage/audition/distance_from_best_precision@10_pct.png]]

** Selecting the best rule or strategy for choosing model groups

In this phase of the audition, you will see what will happen in the
next time if you choose your model group with an specific strategy or
rule.

You then, can calculate the /regret/. /Regret/ is defined as the
difference between the performance of the best model evaluated on the
"next time" and the performance of the model selected by a particular rule.

#+CAPTION: Given a strategy for selecting model groups (in the plot 4 are shown), What will be the performace of the model group chosen by that strategy in the next evaluation date?
#+LABEL: fig:performance-next-time
[[file:~/projects/dsapp/dirtyduck/triage/audition/precision@10_pct_next_time.png]]


#+CAPTION: Given a strategy for selecting model groups (in the plot 4 are shown). What will be the distance (/regret/) to the best theoretical model in the following evaluation date? This plot is similar to the [@fig:distance-from-best]
#+LABEL: fig:regret-from-best-given-a-rule
[[file:~/projects/dsapp/dirtyduck/triage/audition/regret_distance_from_best_rules_precision@10_pct.png]]



#+CAPTION: Expected regret for the strategies. The less the better.
#+LABEL: fig:regret-over-time
[[file:~/projects/dsapp/dirtyduck/triage/audition/regret_over_time_precision@10_pct.png]]


All this information will be stored in the file =results_model_group_ids.json=:

#+INCLUDE: ../triage/audition/results_model_group_ids.json src json

The best strategy (the one with the lower “regret”) for selecting
a =model_group= is =most_frequent_best_dist_precision=


* WORKING Postmodeling: Inspecting the best models closely

/Postmodeling/ will help you to understand the behaviour orf your selected models (from audition)

Compared to the previous sections, /postmodeling/ is not an automated process (yet).

#+BEGIN_SRC yaml :tangle ../triage/eis_postmodeling_config.yaml
# Postmodeling Configuration File

  project_path: '/triage' # Project path defined in triage with matrices and models
  audition_output_path: '/triage/audition/results_model_group_ids.json'



  thresholds: # Thresholds for2 defining positive predictions
        rank_abs: [50, 100, 250]
        rank_pct: [5, 10, 25]

  baseline_query: | # SQL query for defining a baseline for comparison in plots. It needs a metric and parameter
      select g.model_group_id,
             m.model_id,
             extract('year' from m.evaluation_end_time) as as_of_date_year,
             m.metric,
             m.parameter,
             m.value,
             m.num_labeled_examples,
             m.num_labeled_above_threshold,
             m.num_positive_labels
       from test_results.evaluations m
       left join model_metadata.models g
       using(model_id)
       where g.model_group_id = 65
             and metric = 'precision@'
             and parameter = '10_pct'

  max_depth_error_tree: 5 # For error trees, how depth the decision trees should go?
  n_features_plots: 10 # Number of features for importances
  figsize: [12, 12] # Default size for plots
  fontsize: 20 # Default fontsize for plots
#+END_SRC

** Setup

#+BEGIN_SRC sh :dir /docker:root@tutorial_bastion:/root/.local/share/jupyter/runtime/
ls
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
kernel-f8416bdd-d6d6-4d11-afad-5748353b4ba9.json  notebook_cookie_secret
#+END_SRC


#+BEGIN_SRC sh :dir /docker:root@tutorial_bastion:/root/.local/share/jupyter/runtime/
jupyter-kernel –debug −–KernelManager.control_port=56406
#+END_SRC




#+BEGIN_SRC jupyter-python :session /docker:root@tutorial_bastion:/root/.local/share/jupyter/runtime/
import pandas as pd
import numpy as np
from collections import OrderedDict
from utils.aux_funcs import create_pgconn, get_models_ids
from triage.component.catwalk.storage import ProjectStorage, ModelStorageEngine, MatrixStorageEngine
from parameters import PostmodelParameters
from model_evaluator import ModelEvaluator
from model_group_evaluator import ModelGroupEvaluator

params = PostmodelParameters('../triage/eis_postmodeling_config.yaml')
#+END_SRC


** Over time

#+BEGIN_SRC jupyter-python :session /docker:root@tutorial_bastion:/root/.local/share/jupyter/runtime/kernel-f8416bdd-d6d6-4d11-afad-5748353b4ba9.json
print("Hola")
#+END_SRC

#+RESULTS:
: Hola


** Feature group importance

** Higher ratio across labels from =crosstabs=

** Overlap

* WORKING Crossvalidation: How are the entities classified?

#+BEGIN_SRC yaml :tangle ../triage/eis_crosstabs_config.yaml
output:
  schema: 'test_results'
  table: 'eis_crosstabs_test'

thresholds:
    rank_abs: [50]
    rank_pct: [5]

#(optional): a list of entity_ids to subset on the crosstabs analysis
entity_id_list: []

models_list_query: "select unnest(ARRAY[44, 86]) :: int as model_id"

as_of_dates_query: "select unnest(ARRAY['2017-12-01']) :: date as as_of_date"

#don't change this query unless strictly necessary. It is just validating pairs of (model_id,as_of_date)
#it is just a join with distinct (model_id, as_of_date) in a predictions table
models_dates_join_query: |
  select model_id,
  as_of_date
  from models_list_query as m
  cross join as_of_dates_query a join (select distinct model_id, as_of_date from test_results.predictions) as p
  using (model_id, as_of_date)

#features_query must join models_dates_join_query with 1 or more features table using as_of_date
features_query: |
  select m.model_id, f1.*
  from features.inspections_aggregation_imputed as f1 join
  models_dates_join_query as m using (as_of_date)

#the predictions query must return model_id, as_of_date, entity_id, score, label_value, rank_abs and rank_pct
#it must join models_dates_join_query using both model_id and as_of_date
predictions_query: |
  select model_id,
      as_of_date,
      entity_id,
      score,
      label_value,
      coalesce(rank_abs, row_number() over (partition by (model_id, as_of_date) order by score desc)) as rank_abs,
      coalesce(rank_pct*100, ntile(100) over (partition by (model_id, as_of_date) order by score desc)) as rank_pct
      from test_results.predictions
      join models_dates_join_query USING(model_id, as_of_date)
      where model_id IN (select model_id from models_list_query)
      and as_of_date in (select as_of_date from as_of_dates_query)"
#+END_SRC

* Footnotes


[fn:1] /Best/ in the sense of the rules that we defined in the =audition=
config file.
