#+STARTUP: showeverything
#+STARTUP: nohideblocks
#+STARTUP: indent
#+STARTUP: align
#+STARTUP: inlineimages
#+STARTUP: latexpreview
#+PROPERTY: header-args:sql :engine postgresql
#+PROPERTY: header-args:sql+ :dbhost 0.0.0.0
#+PROPERTY: header-args:sql+ :dbport 5434
#+PROPERTY: header-args:sql+ :dbuser food_user
#+PROPERTY: header-args:sql+ :dbpassword some_password
#+PROPERTY: header-args:sql+ :database food
#+PROPERTY: header-args:sql+ :results table drawer
#+PROPERTY: header-args:sql+ :exports both
#+PROPERTY: header-args:sql+ :eval no-export
#+PROPERTY: header-args:sql+ :cmdline -q
#+PROPERTY: header-args:sh  :results verbatim org
#+PROPERTY: header-args:sh+ :prologue exec 2>&1 :epilogue :
#+PROPERTY: header-args:ipython   :session food_inspections
#+PROPERTY: header-args:ipython+ :results raw drawer
#+OPTIONS: broken-links:mark
#+OPTIONS: tasks:todo
#+OPTIONS: LaTeX:t

* WORKING Audition: How can I pick the best one?

Now you have *42* /model groups/. Which is best? Which should you choose to
use? This is not as easy as it sounds, due to several factors:

- You can try to pick the best using a metric
  specified in the config file (=precision@= and =recall@=),
  but at what point of time? Maybe different model groups are best
  at different prediction times.
- You can just use the one that performs best on the last test set.
- You can value a model group that provides consistent results over time.
  It might not be the best on any test set, but you can feel more
  confident that it will continue to perform similarly.
- If there are several model groups that perform similarly and
  their lists are more or less similar, maybe it doesn't really
  matter which you pick.

The answers to questions like these may not be obvious up front.

Triage provides this functionality in =audition= and in
=postmodel=. At the moment of this writing, these two modules require
more interaction (i.e. they aren't integrated with the /configuration
file/).

Audition is a tool for picking the best trained classifiers from a
predictive analytics experiment.  Audition introduces
a structured, semi-automated way of filtering models based on what you
consider important.

=Audition= formalizes this idea through /selection rules/ that take in
the data up to a given point in time, apply some rule to choose a
model group, and then evaluate the performance (*regret*) of the chosen
model group in the subsequent time window.

=Audition= predefines 7 rules:

1. =best_current_value= :: Pick the model group with the best current metric Value.
2. =best_average_value= :: Pick the model with the highest average metric value so far.
3. =lowest_metric_variance= :: Pick the model with the lowest metric variance so far.
4. =most_frequent_best_dist= :: Pick the model that is most frequently
     within =dist_from_best_case= from the best-performing model group
     across test sets so far.
5. =best_average_two_metrics= :: Pick the model with the highest
     average combined value to date of two metrics weighted together
     using =metric1_weight=.
6. =best_avg_var_penalized= :: Pick the model with the highest average
     metric value so far, penalized for relative variance ss:
     =avg_value - (stdev_penalty) * (stdev - min_stdev)= where
     =min_stdev= is the minimum standard deviation of the metric
     across all model groups
7.  =best_avg_recency_weight= :: Pick the model with the highest
     average metric value so far, penalized for relative variance as:
     =avg_value - (stdev_penalty) * (stdev - min_stdev)= where
     =min_stdev= is the minimum standard deviation of the metric
     across all  model groups

We included a simple configuration file with some rules:

#+BEGIN_EXAMPLE yaml :tangle ../triage/audition_config.yaml
# CHOOSE MODEL GROUPS
model_groups:
    query: |
        SELECT DISTINCT(model_group_id)
        FROM results.model_groups
# CHOOSE TIMESTAMPS/TRAIN END TIMES
time_stamps:
    query: |
        SELECT DISTINCT train_end_time
        FROM results.models
        WHERE model_group_id IN ({})
        AND EXTRACT(DAY FROM train_end_time) IN (1)
        AND train_end_time >= '2012-01-01'
# FILTER
filter:
    metric: 'precision@' # metric of interest
    parameter: '50_abs' # parameter of interest
    max_from_best: 1.0 # The maximum value that the given metric can be worse than the best model for a given train end time.
    threshold_value: 0.0 # The worst absolute value that the given metric should be.
    distance_table: 'distance_table' # name of the distance table
    models_table: 'models' # name of the models table

# RULES
rules:
    -
        shared_parameters:
            -
                metric: 'precision@'
                parameter: '50_abs'
            -
                metric: 'recall@'
                parameter: '50_abs'
        selection_rules:
            -
                name: 'best_current_value' # Pick the model group with the best current metric value
                num: 3
            -
                name: 'best_average_value' # Pick the model with the highest average metric value
                num: 3
            -
                name: 'lowest_metric_variance' # Pick the model with the lowest metric variance
                num: 3
            -
                name: 'most_frequent_best_dist' # Pick the model that is most frequently within `dist_from_best_case`
                dist_from_best_case: [0.05]
                num: 3

    -
        shared_parameters:
            -
                metric1: 'precision@'
                parameter1: '50_abs'
        selection_rules:
            -
                name: 'best_average_two_metrics' #  Pick the model with the highest average combined value to date of two metrics weighted together using metric1_weight
                metric2: 'recall@'
                parameter2: '50_abs'
                metric1_weight: 0.5
                num: 3
#+END_EXAMPLE

=Audition= will have each rule give you the best $n$ model-group IDs
based on the metric and parameter following that rule for the most
recent time period (in all the rules shown $n$ = 1).

We can run the simulation of the rules againts the experiment as:

#+BEGIN_SRC sh :dir /docker:root@tutorial_bastion:/triage
triage audition
:
#+END_SRC

#+RESULTS:
#+BEGIN_SRC org
/usr/local/lib/python3.6/site-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use "pip install psycopg2-binary" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.
  """)
INFO:root:Validate!
[33m[ProgrammingError][39m [31m(psycopg2.ProgrammingError) relation "results.model_groups" does not exist
LINE 2: FROM results.model_groups
             ^
 [SQL: 'SELECT DISTINCT(model_group_id)\nFROM results.model_groups\n'] (Background on this error at: http://sqlalche.me/e/f405)[39m
[0m
#+END_SRC


* TODO Crossvalidation

* TODO Postmodeling
