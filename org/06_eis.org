#+STARTUP: showeverything
#+STARTUP: nohideblocks
#+STARTUP: indent
#+STARTUP: align
#+STARTUP: inlineimages
#+STARTUP: latexpreview
#+PROPERTY: header-args:sql :engine postgresql
#+PROPERTY: header-args:sql+ :dbhost 0.0.0.0
#+PROPERTY: header-args:sql+ :dbport 5434
#+PROPERTY: header-args:sql+ :dbuser food_user
#+PROPERTY: header-args:sql+ :dbpassword some_password
#+PROPERTY: header-args:sql+ :database food
#+PROPERTY: header-args:sql+ :results table drawer
#+PROPERTY: header-args:sql+ :exports both
#+PROPERTY: header-args:sql+ :eval no-export
#+PROPERTY: header-args:sql+ :cmdline -q
#+PROPERTY: header-args:sh  :results verbatim org
#+PROPERTY: header-args:sh+ :prologue exec 2>&1 :epilogue :
#+PROPERTY: header-args:ipython   :session food_inspections
#+PROPERTY: header-args:ipython+ :results raw drawer
#+PROPERTY: header-args:yaml :eval no-export
#+OPTIONS: broken-links:mark
#+OPTIONS: tasks:todo
#+OPTIONS: LaTeX:t

* An Early Intervention System

** Problem description

=triage= is designed to also build early warning systems (also called early intervention, EIS).
While there are  several differences between modeling early warnings and inspection
 prioritization, perhaps the biggest is that  the /entity/ is active
(i.e. it is doing stuff for which
 an outcome will happen) in EIS but passive (i.e. inspected)
 in *inspection prioritization*. Among other things, this difference
affects the way the /outcome/ is built.

Here's the question we want to answer:

#+begin_quote
Will my restaurant be inspected in the
/next X period of time?/
#+end_quote

Where $X$ could be 3 days, 2 months, 1 year,
etc.

  Knowing the answer to this question enables you (as the restaurant
  owner or manager) to prepare for the inspection.


** What are the labels? What are the outcomes?

The trick to note is that on any given day there are two possible outcomes:
/the facility was inspected/ and /the facility wasn't inspected/.
Our /outcomes/ table will be larger than in the inspection prioritization example
because we need an /outcome/ for every /active/ facility on every date.
The following image tries to exemplify this reasoning:


#+NAME: fig:outcomes-inspections
#+CAPTION: The image shows three facilities, and next to each, a temporal line with 6 days (0-5). Each dot represents the event (whether an inspection happened). Yellow means the inspection happened (=TRUE= outcome) and blue means it didn't (=FALSE= outcome). Each facility in the image had two inspections, six in total.
#+ATTR_ORG: :width 600 :height 400
#+ATTR_HTML: :width 600 :height 400
#+ATTR_LATEX: :width 400 :height 300
[[./images/outcomes-eis.png]]

Fortunately, =triage= will help us to create this table. The /cohort/
table is the same as the /cohort/ table in the inspection case.


First the usual stuff. Note that we are changing =model_comment= and
=label_definition= (remember that this is used for generating the
/hash/ that differentiates models and model groups).

#+BEGIN_SRC yaml :tangle ../triage/experiments/eis_01.yaml
config_version: 'v6'

model_comment: 'eis: 01'

user_metadata:
  label_definition: 'inspected'
  experiment_type: 'eis'
  description: |
    EIS 01
  purpose: 'model creation'
  org: 'DSaPP'
  team: 'Tutorial'
  author: 'Your name here'
  etl_date: '2019-01-18'

model_group_keys:
  - 'class_path'
  - 'parameters'
  - 'feature_names'
  - 'feature_groups'
  - 'cohort_name'
  - 'state'
  - 'label_name'
  - 'label_timespan'
  - 'training_as_of_date_frequency'
  - 'max_training_history'
  - 'label_definition'
  - 'experiment_type'
  - 'org'
  - 'team'
  - 'author'
  - 'purpose'
  - 'etl_date'

#+END_SRC

For the labels the query is pretty simple, if the facility showed in
the data, it will get a /positive/ outcome, if not they will get a /negative/ outcome

#+BEGIN_SRC yaml :tangle ../triage/experiments/eis_01.yaml
label_config:
  query: |
    select
    entity_id,
    True::integer as outcome
    from semantic.events
    where '{as_of_date}'::timestamp <= date
    and date < '{as_of_date}'::timestamp + interval '{label_timespan}'
    group by entity_id
  include_missing_labels_in_train_as: False
  name: 'inspected'
#+END_SRC

Note the two introduced changes in this block, first, the /outcome/ is
=True= , because all our observations represent /inspected/ facilities
(see discussion above and in particular previous image), second, we
added the line =include_missing_labels_in_train_as: False=. This line
tells =triage= to incorporate all the missing facilities with =False=  as
the /outcome/.

As stated we will use the same configuration block for /cohorts/ that we
used in inspections:

#+BEGIN_SRC yaml :tangle ../triage/experiments/eis_01.yaml
cohort_config:
  query: |
    select entity_id
    from semantic.entities
    where
    daterange(start_time, end_time, '[]') @> '{as_of_date}'::date
  name: 'active_facilities'
#+END_SRC


** Modeling Using Machine Learning

We need to specify the temporal configuration too

**** Temporal configuration
#+BEGIN_SRC yaml :tangle ../triage/experiments/eis_01.yaml
temporal_config:
    feature_start_time: '2010-01-04'
    feature_end_time: '2019-01-01'
    label_start_time: '2015-02-01'
    label_end_time: '2019-01-01'

    model_update_frequency: '1y'
    training_label_timespans: ['1month']
    training_as_of_date_frequencies: '1month'

    test_durations: '1y'
    test_label_timespans: ['1month']
    test_as_of_date_frequencies: '1month'

    max_training_histories: '5y'
#+END_SRC


As before, you can generate the image of the temporal blocks:


#+BEGIN_SRC sh :dir /docker:root@tutorial_bastion:/triage :results silent

# Remember to run this in bastion  NOT in your laptop shell!
triage showtimechops experiments/eis_01.yaml
#+END_SRC


#+CAPTION: Temporal blocks for the Early Warning System. We want to predict the most likely facilities to be inspected in the following month.
#+ATTR_ORG: :width 600 :height 400
#+ATTR_HTML: :width 600 :height 600
#+ATTR_LATEX: :width 400 :height 300
[[./images/eis_01.png]]

**** Features

Regarding the features, we will use the same ones that were used in [[file:inspections.org][inspections prioritization]]:

#+BEGIN_SRC yaml :tangle ../triage/experiments/eis_01.yaml
feature_aggregations:
  -
    prefix: 'inspections'
    from_obj: 'semantic.events'
    knowledge_date_column: 'date'

    aggregates_imputation:
      count:
        type: 'zero_noflag'

    aggregates:
      -
        quantity:
          total: "*"
        metrics:
          - 'count'

    intervals: ['1month', '3month', '6month', '1y', 'all']

    groups:
      - 'entity_id'

  -
    prefix: 'risks'
    from_obj: 'semantic.events'
    knowledge_date_column: 'date'

    categoricals_imputation:
      sum:
        type: 'zero'
      avg:
        type: 'zero'

    categoricals:
      -
        column: 'risk'
        choices: ['low', 'medium', 'high']
        metrics:
          - 'sum'
          - 'avg'

    intervals: ['1month', '3month', '6month', '1y', 'all']

    groups:
      - 'entity_id'
      - 'zip_code'

  -
    prefix: 'results'
    from_obj: 'semantic.events'
    knowledge_date_column: 'date'

    categoricals_imputation:
      all:
        type: 'zero'

    categoricals:
      -
        column: 'result'
        choice_query: 'select distinct result from semantic.events'
        metrics:
          - 'sum'
          - 'avg'

    intervals: ['1month', '3month', '6month', '1y', 'all']

    groups:
      - 'entity_id'

  -
    prefix: 'inspection_types'
    from_obj: 'semantic.events'
    knowledge_date_column: 'date'

    categoricals_imputation:
      sum:
        type: 'zero_noflag'

    categoricals:
      -
        column: 'type'
        choice_query: 'select distinct type from semantic.events where type is not null'
        metrics:
          - 'sum'

    intervals: ['1month', '3month', '6month', '1y', 'all']

    groups:
      - 'entity_id'
      - 'zip_code'

grid_config:
    'sklearn.tree.DecisionTreeClassifier':
        max_depth: [2,10,~]
        min_samples_split: [2,5]


#+END_SRC

We declare that we want to use all possible feature-group combinations for training:

#+BEGIN_SRC yaml :tangle ../triage/experiments/eis_01.yaml
feature_group_definition:
   prefix:
     - 'inspections'
     - 'results'
     - 'risks'
     - 'inspection_types'

feature_group_strategies: ['all', 'leave-one-out']
#+END_SRC

i.e. =all= will train models with all the features groups,
=leave-one-in= will use only one of the feature groups for traning, and
lastly, =leave-one-out= will train the model with all the features
except one.

**** Algorithm and hyperparameters

We will collapse the baseline (=DummyClassifier=) and the exploratory configuration together:

#+BEGIN_SRC yaml :tangle ../triage/experiments/eis_01.yaml
grid_config:
    'sklearn.tree.DecisionTreeClassifier':
        max_depth: [2,null]
    'sklearn.ensemble.RandomForestClassifier':
        max_features: ['sqrt']
        criterion: ['gini']
        n_estimators: [250]
        min_samples_leaf: [1]
        min_samples_split: [50]
    'sklearn.dummy.DummyClassifier':
        strategy: [most_frequent]
#+END_SRC

=triage= will create *20* /model groups/: *4* algorithms and
hyperparameters (2 =DecisionTreeClassifier=, 1
=RandomForestClassifier=, 1 =DummyClassifier=) \times *5* features groups (1
=all=, 4 =leave-one-out=). The total number of /models/
is triple that (we have 3 time blocks, so *60*).

#+BEGIN_SRC yaml :tangle ../triage/experiments/eis_01.yaml
scoring:
    testing_metric_groups:
        -
          metrics: [precision@, recall@]
          thresholds:
            percentiles: [1.0, 2.0, 3.0, 4.0, 5.0, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100]
            top_n: [1, 5, 10, 25, 50, 100, 250, 500, 1000]


    training_metric_groups:
      -
        metrics: [accuracy]
      -
        metrics: [precision@, recall@]
        thresholds:
          percentiles: [1.0, 2.0, 3.0, 4.0, 5.0, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100]
          top_n: [1, 5, 10, 25, 50, 100, 250, 500, 1000]

#+END_SRC

As a last step, we validate that the configuration file is correct:


#+BEGIN_SRC sh :dir /docker:root@tutorial_bastion:/ :results silent

# Remember to run this in bastion  NOT in your laptop shell!
triage experiment experiments/eis_01.yaml  --validate-only
#+END_SRC


And then just run it:

#+BEGIN_SRC sh :dir /docker:root@tutorial_bastion:/ :results silent
# Remember to run this in bastion  NOT in your laptop shell!
triage experiment experiments/eis_01.yaml --profile
#+END_SRC

This will take a *lot* amount of time (on my computer took 3h 42m),
so, grab your coffee, chat with
your coworkers, check your email, or read the [[https://dssg.uchicago.edu/blog][DSSG blog]].
It's taking that long for several reasons:

1. There are a lot of models, parameters, etc.
2. We are running in serial mode (i.e. not in parallel).
3. The database is running on your laptop.

You can solve 2 and 3. For the second point you could use the =docker=
container that has the multicore option enabled. For 3, I recommed you
to use a PostgreSQL database in the cloud, such as Amazon's
*PostgreSQL RDS* (we will explore this later in running triage in AWS Batch).

After the experiment finishes, we can create the following table:

#+BEGIN_SRC sql
with features_groups as (
select
    model_group_id,
    split_part(unnest(feature_list), '_', 1) as feature_groups
from
    model_metadata.model_groups
),

features_arrays as (
select
    model_group_id,
    array_agg(distinct feature_groups) as feature_groups
from
    features_groups
group by
    model_group_id
)

select
    model_group_id,
    model_type,
    hyperparameters,
    feature_groups,
    array_agg(model_id) as models,
    array_agg(train_end_time::date order by train_end_time asc) as times,
    array_agg(to_char(value, '0.999') order by train_end_time asc) as "precision@5%"
from
    model_metadata.models
    join
    features_arrays using(model_group_id)
    join
    test_results.evaluations using(model_id)
where
    model_comment ~ 'eis'
    and
    metric || parameter = 'precision@5.0_pct'
group by
    model_group_id,
    model_type,
    hyperparameters,
    feature_groups
order by
    model_group_id;
#+END_SRC

#+RESULTS:
:RESULTS:
| model_group_id | model_type                               | hyperparameters                                                                                               | feature_groups                          | models | times        | precision@5% |
|--------------+-----------------------------------------+---------------------------------------------------------------------------------------------------------------+----------------------------------------+--------+--------------+--------------|
|           10 | sklearn.tree.DecisionTreeClassifier     | {"max_depth": 2}                                                                                               | {inspection,inspections,results,risks} | {28}   | {2015-12-01} | {" 0.171"}   |
|           11 | sklearn.tree.DecisionTreeClassifier     | {"max_depth": null}                                                                                            | {inspection,inspections,results,risks} | {29}   | {2015-12-01} | {" 0.179"}   |
|           12 | sklearn.ensemble.RandomForestClassifier | {"criterion": "gini", "max_features": "sqrt", "n_estimators": 1000, "min_samples_leaf": 1, "min_samples_split": 50} | {inspection,inspections,results,risks} | {30}   | {2015-12-01} | {" 0.316"}   |
|           13 | sklearn.dummy.DummyClassifier           | {"strategy": "most_frequent"}                                                                                  | {inspection,inspections,results,risks} | {31}   | {2015-12-01} | {" 0.063"}   |
|           14 | sklearn.tree.DecisionTreeClassifier     | {"max_depth": 2}                                                                                               | {inspections}                          | {32}   | {2015-12-01} | {" 0.102"}   |
|           15 | sklearn.tree.DecisionTreeClassifier     | {"max_depth": null}                                                                                            | {inspections}                          | {33}   | {2015-12-01} | {" 0.116"}   |
|           16 | sklearn.ensemble.RandomForestClassifier | {"criterion": "gini", "max_features": "sqrt", "n_estimators": 1000, "min_samples_leaf": 1, "min_samples_split": 50} | {inspections}                          | {34}   | {2015-12-01} | {" 0.116"}   |
|           17 | sklearn.dummy.DummyClassifier           | {"strategy": "most_frequent"}                                                                                  | {inspections}                          | {35}   | {2015-12-01} | {" 0.063"}   |
|           18 | sklearn.tree.DecisionTreeClassifier     | {"max_depth": 2}                                                                                               | {results}                              | {36}   | {2015-12-01} | {" 0.172"}   |
|           19 | sklearn.tree.DecisionTreeClassifier     | {"max_depth": null}                                                                                            | {results}                              | {37}   | {2015-12-01} | {" 0.232"}   |
|           20 | sklearn.ensemble.RandomForestClassifier | {"criterion": "gini", "max_features": "sqrt", "n_estimators": 1000, "min_samples_leaf": 1, "min_samples_split": 50} | {results}                              | {38}   | {2015-12-01} | {" 0.267"}   |
|           21 | sklearn.dummy.DummyClassifier           | {"strategy": "most_frequent"}                                                                                  | {results}                              | {39}   | {2015-12-01} | {" 0.063"}   |
|           22 | sklearn.tree.DecisionTreeClassifier     | {"max_depth": 2}                                                                                               | {risks}                                | {40}   | {2015-12-01} | {" 0.164"}   |
|           23 | sklearn.tree.DecisionTreeClassifier     | {"max_depth": null}                                                                                            | {risks}                                | {41}   | {2015-12-01} | {" 0.126"}   |
|           24 | sklearn.ensemble.RandomForestClassifier | {"criterion": "gini", "max_features": "sqrt", "n_estimators": 1000, "min_samples_leaf": 1, "min_samples_split": 50} | {risks}                                | {42}   | {2015-12-01} | {" 0.226"}   |
|           25 | sklearn.dummy.DummyClassifier           | {"strategy": "most_frequent"}                                                                                  | {risks}                                | {43}   | {2015-12-01} | {" 0.063"}   |
|           26 | sklearn.tree.DecisionTreeClassifier     | {"max_depth": 2}                                                                                               | {inspection}                           | {44}   | {2015-12-01} | {" 0.152"}   |
|           27 | sklearn.tree.DecisionTreeClassifier     | {"max_depth": null}                                                                                            | {inspection}                           | {45}   | {2015-12-01} | {" 0.135"}   |
|           28 | sklearn.ensemble.RandomForestClassifier | {"criterion": "gini", "max_features": "sqrt", "n_estimators": 1000, "min_samples_leaf": 1, "min_samples_split": 50} | {inspection}                           | {46}   | {2015-12-01} | {" 0.213"}   |
|           29 | sklearn.dummy.DummyClassifier           | {"strategy": "most_frequent"}                                                                                  | {inspection}                           | {47}   | {2015-12-01} | {" 0.063"}   |
|           30 | sklearn.tree.DecisionTreeClassifier     | {"max_depth": 2}                                                                                               | {inspection,results,risks}             | {48}   | {2015-12-01} | {" 0.172"}   |
|           31 | sklearn.tree.DecisionTreeClassifier     | {"max_depth": null}                                                                                            | {inspection,results,risks}             | {49}   | {2015-12-01} | {" 0.175"}   |
|           32 | sklearn.ensemble.RandomForestClassifier | {"criterion": "gini", "max_features": "sqrt", "n_estimators": 1000, "min_samples_leaf": 1, "min_samples_split": 50} | {inspection,results,risks}             | {50}   | {2015-12-01} | {" 0.315"}   |
|           33 | sklearn.dummy.DummyClassifier           | {"strategy": "most_frequent"}                                                                                  | {inspection,results,risks}             | {51}   | {2015-12-01} | {" 0.063"}   |
:END:
