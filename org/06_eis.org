#+STARTUP: showeverything
#+STARTUP: nohideblocks
#+STARTUP: indent
#+STARTUP: align
#+STARTUP: inlineimages
#+STARTUP: latexpreview
#+PROPERTY: header-args:sql :engine postgresql
#+PROPERTY: header-args:sql+ :dbhost 0.0.0.0
#+PROPERTY: header-args:sql+ :dbport 5434
#+PROPERTY: header-args:sql+ :dbuser food_user
#+PROPERTY: header-args:sql+ :dbpassword some_password
#+PROPERTY: header-args:sql+ :database food
#+PROPERTY: header-args:sql+ :results table drawer
#+PROPERTY: header-args:sql+ :exports both
#+PROPERTY: header-args:sql+ :eval no-export
#+PROPERTY: header-args:sql+ :cmdline -q
#+PROPERTY: header-args:sh  :results verbatim org
#+PROPERTY: header-args:sh+ :prologue exec 2>&1 :epilogue :
#+PROPERTY: header-args:ipython   :session food_inspections
#+PROPERTY: header-args:ipython+ :results raw drawer
#+PROPERTY: header-args:yaml :eval no-export
#+OPTIONS: broken-links:mark
#+OPTIONS: tasks:todo
#+OPTIONS: LaTeX:t

* An Early Intervention System

** Problem description

=triage= is designed to also build early warning systems (also called early intervention, EIS).
While there are  several differences between modeling early warnings and inspection
 prioritization, perhaps the biggest is that  the /entity/ is active
(i.e. it is doing stuff for which
 an outcome will happen) in EIS but passive (i.e. inspected)
 in *inspection prioritization*. Among other things, this difference
affects the way the /outcome/ is built.

Here's the question we want to answer:

#+begin_quote
Will my restaurant be inspected in the
/next X period of time?/
#+end_quote

Where $X$ could be 3 days, 2 months, 1 year,
etc.

  Knowing the answer to this question enables you (as the restaurant
  owner or manager) to prepare for the inspection.


** What are the labels? What are the outcomes?

The trick to note is that on any given day there are two possible outcomes:
/the facility was inspected/ and /the facility wasn't inspected/.
Our /outcomes/ table will be larger than in the inspection prioritization example
because we need an /outcome/ for every /active/ facility on every date.
The following image tries to exemplify this reasoning:


#+NAME: fig:outcomes-inspections
#+CAPTION: The image shows three facilities, and next to each, a temporal line with 6 days (0-5). Each dot represents the event (whether an inspection happened). Yellow means the inspection happened (=TRUE= outcome) and blue means it didn't (=FALSE= outcome). Each facility in the image had two inspections, six in total.
#+ATTR_ORG: :width 600 :height 400
#+ATTR_HTML: :width 600 :height 400
#+ATTR_LATEX: :width 400 :height 300
[[./images/outcomes-eis.png]]

Fortunately, =triage= will help us to create this table. The /cohort/
table is the same as the /cohort/ table in the inspection case.


First the usual stuff. Note that we are changing =model_comment= and
=label_definition= (remember that this is used for generating the
/hash/ that differentiates models and model groups).

#+BEGIN_SRC yaml :tangle ../triage/experiments/eis_01.yaml
config_version: 'v6'

model_comment: 'eis: 01'

user_metadata:
  label_definition: 'inspected'
  experiment_type: 'eis'
  description: |
    EIS 01
  purpose: 'model creation'
  org: 'DSaPP'
  team: 'Tutorial'
  author: 'Your name here'
  etl_date: '2019-02-21'

model_group_keys:
  - 'class_path'
  - 'parameters'
  - 'feature_names'
  - 'feature_groups'
  - 'cohort_name'
  - 'state'
  - 'label_name'
  - 'label_timespan'
  - 'training_as_of_date_frequency'
  - 'max_training_history'
  - 'label_definition'
  - 'experiment_type'
  - 'org'
  - 'team'
  - 'author'
  - 'purpose'
  - 'etl_date'

#+END_SRC

For the labels the query is pretty simple, if the facility showed in
the data, it will get a /positive/ outcome, if not they will get a /negative/ outcome

#+BEGIN_SRC yaml :tangle ../triage/experiments/eis_01.yaml
label_config:
  query: |
    select
    entity_id,
    True::integer as outcome
    from semantic.events
    where '{as_of_date}'::timestamp <= date
    and date < '{as_of_date}'::timestamp + interval '{label_timespan}'
    group by entity_id
  include_missing_labels_in_train_as: False
  name: 'inspected'
#+END_SRC

Note the two introduced changes in this block, first, the /outcome/ is
=True= , because all our observations represent /inspected/ facilities
(see discussion above and in particular previous image), second, we
added the line =include_missing_labels_in_train_as: False=. This line
tells =triage= to incorporate all the missing facilities with =False=  as
the /outcome/.

As stated we will use the same configuration block for /cohorts/ that we
used in inspections:

#+BEGIN_SRC yaml :tangle ../triage/experiments/eis_01.yaml
cohort_config:
  query: |
    select entity_id
    from semantic.entities
    where
    daterange(start_time, end_time, '[]') @> '{as_of_date}'::date
  name: 'active_facilities'
#+END_SRC


** Modeling Using Machine Learning

We need to specify the temporal configuration too

**** Temporal configuration
#+BEGIN_SRC yaml :tangle ../triage/experiments/eis_01.yaml
temporal_config:
    feature_start_time: '2010-01-04'
    feature_end_time: '2019-01-01'
    label_start_time: '2015-02-01'
    label_end_time: '2019-01-01'

    model_update_frequency: '1y'
    training_label_timespans: ['1month']
    training_as_of_date_frequencies: '1month'

    test_durations: '1y'
    test_label_timespans: ['1month']
    test_as_of_date_frequencies: '1month'

    max_training_histories: '5y'
#+END_SRC


As before, you can generate the image of the temporal blocks:


#+BEGIN_SRC sh :dir /docker:root@tutorial_bastion:/triage :results silent
# Remember to run this in bastion  NOT in your laptop shell!
triage experiment experiments/eis_01.yaml --show-timechop
#+END_SRC


#+CAPTION: Temporal blocks for the Early Warning System. We want to predict the most likely facilities to be inspected in the following month.
#+ATTR_ORG: :width 600 :height 400
#+ATTR_HTML: :width 600 :height 600
#+ATTR_LATEX: :width 400 :height 300
[[./images/eis_01.png]]

**** Features

Regarding the features, we will use the same ones that were used in [[file:inspections.org][inspections prioritization]]:

#+BEGIN_SRC yaml :tangle ../triage/experiments/eis_01.yaml
feature_aggregations:
  -
    prefix: 'inspections'
    from_obj: 'semantic.events'
    knowledge_date_column: 'date'

    aggregates_imputation:
      count:
        type: 'zero_noflag'

    aggregates:
      -
        quantity:
          total: "*"
        metrics:
          - 'count'

    intervals: ['1month', '3month', '6month', '1y', 'all']

    groups:
      - 'entity_id'

  -
    prefix: 'risks'
    from_obj: 'semantic.events'
    knowledge_date_column: 'date'

    categoricals_imputation:
      sum:
        type: 'zero'
      avg:
        type: 'zero'

    categoricals:
      -
        column: 'risk'
        choices: ['low', 'medium', 'high']
        metrics:
          - 'sum'
          - 'avg'

    intervals: ['1month', '3month', '6month', '1y', 'all']

    groups:
      - 'entity_id'
      - 'zip_code'

  -
    prefix: 'results'
    from_obj: 'semantic.events'
    knowledge_date_column: 'date'

    categoricals_imputation:
      all:
        type: 'zero'

    categoricals:
      -
        column: 'result'
        choice_query: 'select distinct result from semantic.events'
        metrics:
          - 'sum'
          - 'avg'

    intervals: ['1month', '3month', '6month', '1y', 'all']

    groups:
      - 'entity_id'

  -
    prefix: 'inspection_types'
    from_obj: 'semantic.events'
    knowledge_date_column: 'date'

    categoricals_imputation:
      sum:
        type: 'zero_noflag'

    categoricals:
      -
        column: 'type'
        choice_query: 'select distinct type from semantic.events where type is not null'
        metrics:
          - 'sum'

    intervals: ['1month', '3month', '6month', '1y', 'all']

    groups:
      - 'entity_id'
      - 'zip_code'


#+END_SRC

We declare that we want to use all possible feature-group combinations for training:

#+BEGIN_SRC yaml :tangle ../triage/experiments/eis_01.yaml
feature_group_definition:
   prefix:
     - 'inspections'
     - 'results'
     - 'risks'
     - 'inspection_types'

feature_group_strategies: ['all', 'leave-one-out', 'leave-one-in']
#+END_SRC

i.e. =all= will train models with all the features groups,
=leave-one-in= will use only one of the feature groups for traning, and
lastly, =leave-one-out= will train the model with all the features
except one.

**** Algorithm and hyperparameters

We will collapse the baseline (=DummyClassifier=) and the exploratory configuration together:

#+BEGIN_SRC yaml :tangle ../triage/experiments/eis_01.yaml
grid_config:
    'sklearn.tree.DecisionTreeClassifier':
        max_depth: [2,null]
    'sklearn.ensemble.RandomForestClassifier':
        max_features: ['sqrt']
        criterion: ['gini']
        n_estimators: [500]
        min_samples_leaf: [1]
        min_samples_split: [50]
    'sklearn.dummy.DummyClassifier':
        strategy: [most_frequent]
#+END_SRC

=triage= will create *36* /model groups/: *4* algorithms and
hyperparameters (2 =DecisionTreeClassifier=, 1
=RandomForestClassifier=, 1 =DummyClassifier=) \times *9* features sets (1
=all=, 4 =leave-one-out=, =4 leave-one-in=). The total number of /models/
is three times that (we have 3 time blocks, so *108* models).

#+BEGIN_SRC yaml :tangle ../triage/experiments/eis_01.yaml
scoring:
    testing_metric_groups:
        -
          metrics: [precision@, recall@]
          thresholds:
            percentiles: [1.0, 2.0, 3.0, 4.0, 5.0, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100]
            top_n: [1, 5, 10, 25, 50, 100, 250, 500, 1000]


    training_metric_groups:
      -
        metrics: [accuracy]
      -
        metrics: [precision@, recall@]
        thresholds:
          percentiles: [1.0, 2.0, 3.0, 4.0, 5.0, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100]
          top_n: [1, 5, 10, 25, 50, 100, 250, 500, 1000]

#+END_SRC

As a last step, we validate that the configuration file is correct:


#+BEGIN_SRC sh :dir /docker:root@tutorial_bastion:/ :results silent

# Remember to run this in bastion  NOT in your laptop shell!
triage experiment experiments/eis_01.yaml  --validate-only
#+END_SRC


And then just run it:

#+BEGIN_SRC sh :dir /docker:root@tutorial_bastion:/ :results silent
# Remember to run this in bastion  NOT in your laptop shell!
triage experiment --matrix-format hdf experiments/eis_01.yaml --no-save-predictions --profile
#+END_SRC

This will take a *lot* amount of time (on my computer took 3h 42m),
so, grab your coffee, chat with
your coworkers, check your email, or read the [[https://dssg.uchicago.edu/blog][DSSG blog]].
It's taking that long for several reasons:

1. There are a lot of models, parameters, etc.
2. We are running in serial mode (i.e. not in parallel).
3. The database is running on your laptop.

You can solve 2 and 3. For the second point you could use the =docker=
container that has the multicore option enabled. For 3, I recommed you
to use a PostgreSQL database in the cloud, such as Amazon's
*PostgreSQL RDS* (we will explore this later in running triage in AWS Batch).

After the experiment finishes, we can create the following table:

#+BEGIN_SRC sql
with features_groups as (
select
    model_group_id,
    split_part(unnest(feature_list), '_', 1) as feature_groups
from
    model_metadata.model_groups
),

features_arrays as (
select
    model_group_id,
    array_agg(distinct feature_groups) as feature_groups
from
    features_groups
group by
    model_group_id
)

select
    model_group_id,
    model_type,
    hyperparameters,
    feature_groups,
    array_agg(model_id) as models,
    array_agg(train_end_time::date order by train_end_time asc) as times,
    array_agg(to_char(value, '0.999') order by train_end_time asc) as "precision@10%"
from
    model_metadata.models
    join
    features_arrays using(model_group_id)
    join
    test_results.evaluations using(model_id)
where
    model_comment ~ 'eis'
    and
    metric || parameter = 'precision@10_pct'
group by
    model_group_id,
    model_type,
    hyperparameters,
    feature_groups
order by
    model_group_id;
#+END_SRC

#+RESULTS:
:RESULTS:
| model_group_id | model_type                               | hyperparameters                                                                                              | feature_groups                          | models        | times                              | precision@10%                |
|--------------+-----------------------------------------+--------------------------------------------------------------------------------------------------------------+----------------------------------------+---------------+------------------------------------+------------------------------|
|           46 | sklearn.tree.DecisionTreeClassifier     | {"max_depth": 2}                                                                                              | {inspection,inspections,results,risks} | {208,172,136} | {2015-12-01,2016-12-01,2017-12-01} | {" 0.131"," 0.125"," 0.104"} |
|           47 | sklearn.tree.DecisionTreeClassifier     | {"max_depth": null}                                                                                           | {inspection,inspections,results,risks} | {209,173,137} | {2015-12-01,2016-12-01,2017-12-01} | {" 0.153"," 0.153"," 0.113"} |
|           48 | sklearn.ensemble.RandomForestClassifier | {"criterion": "gini", "max_features": "sqrt", "n_estimators": 500, "min_samples_leaf": 1, "min_samples_split": 50} | {inspection,inspections,results,risks} | {174,138,210} | {2015-12-01,2016-12-01,2017-12-01} | {" 0.244"," 0.239"," 0.168"} |
|           49 | sklearn.dummy.DummyClassifier           | {"strategy": "most_frequent"}                                                                                 | {inspection,inspections,results,risks} | {175,139,211} | {2015-12-01,2016-12-01,2017-12-01} | {" 0.063"," 0.057"," 0.042"} |
|           50 | sklearn.tree.DecisionTreeClassifier     | {"max_depth": 2}                                                                                              | {inspection,results,risks}             | {212,140,176} | {2015-12-01,2016-12-01,2017-12-01} | {" 0.138"," 0.137"," 0.104"} |
|           51 | sklearn.tree.DecisionTreeClassifier     | {"max_depth": null}                                                                                           | {inspection,results,risks}             | {177,141,213} | {2015-12-01,2016-12-01,2017-12-01} | {" 0.148"," 0.153"," 0.113"} |
|           52 | sklearn.ensemble.RandomForestClassifier | {"criterion": "gini", "max_features": "sqrt", "n_estimators": 500, "min_samples_leaf": 1, "min_samples_split": 50} | {inspection,results,risks}             | {178,142,214} | {2015-12-01,2016-12-01,2017-12-01} | {" 0.245"," 0.240"," 0.169"} |
|           53 | sklearn.dummy.DummyClassifier           | {"strategy": "most_frequent"}                                                                                 | {inspection,results,risks}             | {143,179,215} | {2015-12-01,2016-12-01,2017-12-01} | {" 0.063"," 0.057"," 0.042"} |
|           54 | sklearn.tree.DecisionTreeClassifier     | {"max_depth": 2}                                                                                              | {inspection,inspections,risks}         | {216,180,144} | {2015-12-01,2016-12-01,2017-12-01} | {" 0.145"," 0.124"," 0.107"} |
|           55 | sklearn.tree.DecisionTreeClassifier     | {"max_depth": null}                                                                                           | {inspection,inspections,risks}         | {217,145,181} | {2015-12-01,2016-12-01,2017-12-01} | {" 0.121"," 0.124"," 0.087"} |
|           56 | sklearn.ensemble.RandomForestClassifier | {"criterion": "gini", "max_features": "sqrt", "n_estimators": 500, "min_samples_leaf": 1, "min_samples_split": 50} | {inspection,inspections,risks}         | {146,218,182} | {2015-12-01,2016-12-01,2017-12-01} | {" 0.205"," 0.198"," 0.136"} |
|           57 | sklearn.dummy.DummyClassifier           | {"strategy": "most_frequent"}                                                                                 | {inspection,inspections,risks}         | {219,147,183} | {2015-12-01,2016-12-01,2017-12-01} | {" 0.063"," 0.057"," 0.042"} |
|           58 | sklearn.tree.DecisionTreeClassifier     | {"max_depth": 2}                                                                                              | {inspection,inspections,results}       | {148,184,220} | {2015-12-01,2016-12-01,2017-12-01} | {" 0.131"," 0.125"," 0.098"} |
|           59 | sklearn.tree.DecisionTreeClassifier     | {"max_depth": null}                                                                                           | {inspection,inspections,results}       | {185,221,149} | {2015-12-01,2016-12-01,2017-12-01} | {" 0.147"," 0.140"," 0.111"} |
|           60 | sklearn.ensemble.RandomForestClassifier | {"criterion": "gini", "max_features": "sqrt", "n_estimators": 500, "min_samples_leaf": 1, "min_samples_split": 50} | {inspection,inspections,results}       | {222,186,150} | {2015-12-01,2016-12-01,2017-12-01} | {" 0.227"," 0.220"," 0.154"} |
|           61 | sklearn.dummy.DummyClassifier           | {"strategy": "most_frequent"}                                                                                 | {inspection,inspections,results}       | {187,223,151} | {2015-12-01,2016-12-01,2017-12-01} | {" 0.063"," 0.057"," 0.042"} |
|           62 | sklearn.tree.DecisionTreeClassifier     | {"max_depth": 2}                                                                                              | {inspections,results,risks}            | {152,188,224} | {2015-12-01,2016-12-01,2017-12-01} | {" 0.131"," 0.125"," 0.104"} |
|           63 | sklearn.tree.DecisionTreeClassifier     | {"max_depth": null}                                                                                           | {inspections,results,risks}            | {189,153,225} | {2015-12-01,2016-12-01,2017-12-01} | {" 0.152"," 0.152"," 0.105"} |
|           64 | sklearn.ensemble.RandomForestClassifier | {"criterion": "gini", "max_features": "sqrt", "n_estimators": 500, "min_samples_leaf": 1, "min_samples_split": 50} | {inspections,results,risks}            | {190,154,226} | {2015-12-01,2016-12-01,2017-12-01} | {" 0.242"," 0.238"," 0.163"} |
|           65 | sklearn.dummy.DummyClassifier           | {"strategy": "most_frequent"}                                                                                 | {inspections,results,risks}            | {191,227,155} | {2015-12-01,2016-12-01,2017-12-01} | {" 0.063"," 0.057"," 0.042"} |
|           66 | sklearn.tree.DecisionTreeClassifier     | {"max_depth": 2}                                                                                              | {inspections}                          | {192,156,228} | {2015-12-01,2016-12-01,2017-12-01} | {" 0.101"," 0.102"," 0.075"} |
|           67 | sklearn.tree.DecisionTreeClassifier     | {"max_depth": null}                                                                                           | {inspections}                          | {193,229,157} | {2015-12-01,2016-12-01,2017-12-01} | {" 0.109"," 0.108"," 0.087"} |
|           68 | sklearn.ensemble.RandomForestClassifier | {"criterion": "gini", "max_features": "sqrt", "n_estimators": 500, "min_samples_leaf": 1, "min_samples_split": 50} | {inspections}                          | {230,194,158} | {2015-12-01,2016-12-01,2017-12-01} | {" 0.109"," 0.108"," 0.087"} |
|           69 | sklearn.dummy.DummyClassifier           | {"strategy": "most_frequent"}                                                                                | {inspections}                          | {231,159,195} | {2015-12-01,2016-12-01,2017-12-01} | {" 0.063"," 0.057"," 0.042"} |
|           70 | sklearn.tree.DecisionTreeClassifier     | {"max_depth": 2}                                                                                              | {results}                              | {160,196,232} | {2015-12-01,2016-12-01,2017-12-01} | {" 0.130"," 0.126"," 0.096"} |
|           71 | sklearn.tree.DecisionTreeClassifier     | {"max_depth": null}                                                                                           | {results}                              | {233,197,161} | {2015-12-01,2016-12-01,2017-12-01} | {" 0.192"," 0.189"," 0.134"} |
|           72 | sklearn.ensemble.RandomForestClassifier | {"criterion": "gini", "max_features": "sqrt", "n_estimators": 500, "min_samples_leaf": 1, "min_samples_split": 50} | {results}                              | {162,198,234} | {2015-12-01,2016-12-01,2017-12-01} | {" 0.213"," 0.205"," 0.144"} |
|           73 | sklearn.dummy.DummyClassifier           | {"strategy": "most_frequent"}                                                                                 | {results}                              | {199,163,235} | {2015-12-01,2016-12-01,2017-12-01} | {" 0.063"," 0.057"," 0.042"} |
|           74 | sklearn.tree.DecisionTreeClassifier     | {"max_depth": 2}                                                                                              | {risks}                                | {164,200,236} | {2015-12-01,2016-12-01,2017-12-01} | {" 0.165"," 0.152"," 0.107"} |
|           75 | sklearn.tree.DecisionTreeClassifier     | {"max_depth": null}                                                                                           | {risks}                                | {201,237,165} | {2015-12-01,2016-12-01,2017-12-01} | {" 0.125"," 0.116"," 0.083"} |
|           76 | sklearn.ensemble.RandomForestClassifier | {"criterion": "gini", "max_features": "sqrt", "n_estimators": 500, "min_samples_leaf": 1, "min_samples_split": 50} | {risks}                                | {238,202,166} | {2015-12-01,2016-12-01,2017-12-01} | {" 0.199"," 0.193"," 0.130"} |
|           77 | sklearn.dummy.DummyClassifier           | {"strategy": "most_frequent"}                                                                                 | {risks}                                | {239,203,167} | {2015-12-01,2016-12-01,2017-12-01} | {" 0.063"," 0.057"," 0.042"} |
|           78 | sklearn.tree.DecisionTreeClassifier     | {"max_depth": 2}                                                                                              | {inspection}                           | {204,240,168} | {2015-12-01,2016-12-01,2017-12-01} | {" 0.149"," 0.129"," 0.088"} |
|           79 | sklearn.tree.DecisionTreeClassifier     | {"max_depth": null}                                                                                           | {inspection}                           | {241,169,205} | {2015-12-01,2016-12-01,2017-12-01} | {" 0.121"," 0.120"," 0.087"} |
|           80 | sklearn.ensemble.RandomForestClassifier | {"criterion": "gini", "max_features": "sqrt", "n_estimators": 500, "min_samples_leaf": 1, "min_samples_split": 50} | {inspection}                           | {242,206,170} | {2015-12-01,2016-12-01,2017-12-01} | {" 0.184"," 0.178"," 0.128"} |
|           81 | sklearn.dummy.DummyClassifier           | {"strategy": "most_frequent"}                                                                                 | {inspection}                           | {243,207,171} | {2015-12-01,2016-12-01,2017-12-01} | {" 0.063"," 0.057"," 0.042"} |
:END:
